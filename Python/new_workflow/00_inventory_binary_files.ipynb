{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program Overview\n",
    "\n",
    "\n",
    "\n",
    "# Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darwin\n",
      "HOME -> /Users/thompsong\n",
      "Developer -> /Users/thompsong/Developer\n",
      "repodir -> /Users/thompsong/Developer/KSCRocketSeismoHydrology/Python/new_workflow\n",
      "work -> /Users/thompsong/work\n",
      "local_outdir -> /Users/thompsong/work/PROJECTS/KSC_EROSION\n",
      "DROPBOX_TOP -> /Users/thompsong/Dropbox\n",
      "new_data -> /Users/thompsong/data/KSCwell\n",
      "DROPBOX_DATA_TOP -> /Users/thompsong/Dropbox/PROFESSIONAL/RESEARCH/3_Project_Documents/NASAprojects/201602_Rocket_Seismology/DATA/2022_DATA\n",
      "dropbox_outdir -> /Users/thompsong/Dropbox/PROFESSIONAL/RESEARCH/3_Project_Documents/NASAprojects/201602_Rocket_Seismology/DATA/2022_DATA/new_workflow\n",
      "WELLDATA_TOP -> /Users/thompsong/Dropbox/PROFESSIONAL/RESEARCH/3_Project_Documents/NASAprojects/201602_Rocket_Seismology/DATA/2022_DATA/WellData\n",
      "TOB3_DIR -> /Users/thompsong/Dropbox/PROFESSIONAL/RESEARCH/3_Project_Documents/NASAprojects/201602_Rocket_Seismology/DATA/2022_DATA/WellData/Uploads\n",
      "transducersCSVfile -> /Users/thompsong/Developer/KSCRocketSeismoHydrology/Python/new_workflow/transducer_metadata_old.csv\n"
     ]
    }
   ],
   "source": [
    "import header\n",
    "paths = header.setup_environment()\n",
    "for k,v in paths.items():\n",
    "    print(k, '->', v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4q/5426zn1d72vgcb6h658n_8800000gn/T/ipykernel_67655/976997968.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport obspy\\nfrom IPython.display import clear_output\\nFILELOOKUPCSV = os.path.join(paths[\\'new_data\\'], \\'reverse_lookup.csv\\')\\nfilesnotprocessedfile = os.path.join(paths[\\'new_data\\'], \\'files_not_processed.txt\\')\\nMASTERCSV = os.path.join(paths[\\'new_data\\'], f\\'data_inventory.csv\\')\\nerase=False\\nif erase:\\n    os.system(f\\'rm -rf {paths[\\'new_data\\']}/*\\')\\nif not os.path.isfile(FILELOOKUPCSV):\\n    os.system(f\\'echo \"fullpath, outpklfullpath, basename, starttime, endtime, seqno\" > {FILELOOKUPCSV}\\')\\nlookupdf = pd.read_csv(FILELOOKUPCSV)\\nif os.path.isfile(MASTERCSV):\\n    allmasterrows = pd.read_csv(MASTERCSV).to_dict(\\'records\\')\\nelse:\\n    allmasterrows=[]\\n    \\nimport libWellData as LLE\\ntransducersDF = pd.read_csv(paths[\\'transducersCSVfile\\'])\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef copy_to_raw(df, pklfile, to_csv=True):\\n    print(\\'- copying data to %s\\' % pklfile)       \\n    if to_csv:       \\n        df.to_csv(pklfile.replace(\\'.pkl\\', \\'.csv\\'), index=False)\\n    else:\\n        df.to_pickle(pklfile)\\n\\ndef write_masterdf(allmasterrows):\\n    if len(allmasterrows)>0:\\n        masterdf = pd.DataFrame(allmasterrows)\\n        print(f\\'Writing/updating {MASTERCSV}\\')\\n        if \\'starttime\\' in masterdf.columns:\\n            masterdf.sort_values(by=[\\'starttime\\',\\'uploadfolder\\',\\'sampratefolder\\',\\'samprate\\']).to_csv(MASTERCSV, index=False)\\n        else:\\n            masterdf.to_csv(MASTERCSV, index=False)\\n\\ndef process_file(dirpath, file, filenum, load=False):\\n    clear_output()\\n    print(f\\'Processing file {filenum}: {file} in {dirpath}\\')\\n    fparts = file.split(\\'.\\')\\n    if len(fparts)==4:\\n        uploadfolder, sampratefolder, basefilename, ext = fparts\\n    elif len(fparts)==2:\\n        uploadfolder = \\'unknown\\'\\n        sampratefolder = \\'unknown\\'\\n        basefilename, ext = fparts\\n        dparts = dirpath.split(\\'/\\')\\n        if len(dparts)>3:\\n            uploadfolder = dparts[-2]\\n            sampratefolder = dparts[-1]\\n    try:\\n        sampratefolder1, middlename, realsamprateandseqno = basefilename.split(\\'_\\')\\n    except:\\n        print(\\'Failed to split: \\',basefilename)\\n        return \\'failed to split basename\\'\\n    if sampratefolder.lower() == sampratefolder1.lower(): # not a Baro file\\n        if \\'Hz\\' in realsamprateandseqno:\\n            realsamprate, seqno = realsamprateandseqno.split(\\'Hz\\')\\n        elif \\'Sec\\' in realsamprateandseqno:\\n            realsamprate, seqno = realsamprateandseqno.split(\\'Sec\\')\\n        else:\\n            print(f\\'Did not find Hz or Sec in filename: {file}\\')\\n            return \\'Did not find Hz or Sec in filename\\'\\n    elif sampratefolder == \\'Baro\\':\\n        realsamprate, baro, seqno = basefilename.split(\\'_\\')\\n        realsamprate = realsamprate.split(\\'hz\\')[0]\\n        seqno = seqno.split(\\'Sensors\\')[-1]\\n    else:\\n        print(f\\'samprate do not match: {file}, {sampratefolder}, {sampratefolder1}\\' +\\'\\n\\')\\n        return \\'samprate do not match\\'\\n    #print(realsamprate, seqno)\\n    #masterrow={\\'filename\\':os.path.basename(file), \\'topdir\\':dirpath, \\'uploadfolder\\':os.path.basename(uploadfolder), \\'sampratefolder\\':sampratefolder,     #           \\'basename\\':basefilename, \\'samprate\\':realsamprate, \\'seqno\\':seqno} \\n    masterrow={\\'filename\\':file, \\'topdir\\':dirpath, \\'uploadfolder\\':uploadfolder, \\'sampratefolder\\':sampratefolder,                \\'basename\\':basefilename, \\'samprate\\':realsamprate, \\'seqno\\':seqno} \\n\\n\\n    if load:\\n        dropped_headers = False\\n        dropped_rows = 0\\n        fullpath = os.path.join(dirpath, file)\\n        print(f\\'Loading {fullpath}\\')\\n        try:\\n            if file.endswith(\\'pkl\\'):\\n                df = pd.read_pickle(fullpath)\\n            elif file.endswith(\\'csv\\'):\\n                df = pd.read_csv(fullpath)\\n        except Exception as e:\\n            print(e)\\n            os.system(f\\'head {fullpath}\\')\\n            raise e\\n        \\n        # Note that converted TOB3 files have multiple header lines and columns are read as dtype \"object\" because of mixed dtype. So we have to explicity convert them after removing excess header rows.\\n        #    The first header row is garbage. We want the second\\n\\n\\n        # Drop incorrect header row\\n        columns_old = []\\n        if not \\'TIMESTAMP\\' in df.columns: # use 2nd row of file/0th row of dataframe for columns instead\\n            columns_old = df.columns\\n            df.columns = df.iloc[0]\\n            df=df[1:]\\n            dropped_headers=True\\n            \\n        # Convert TIMESTAMP\\n        df[\\'TIMESTAMP\\'] = pd.to_datetime(df[\\'TIMESTAMP\\'], format=\\'ISO8601\\', errors=\\'coerce\\')\\n        l1 = len(df)\\n        df = df.dropna(subset=[\\'TIMESTAMP\\'])\\n\\n        # filter by TIMESTAMP further\\n        TS_median = df[\\'TIMESTAMP\\'].median()\\n        TS_start = TS_median - pd.Timedelta(hours=4)\\n        TS_end = TS_median + pd.Timedelta(hours=4)\\n        df = df[(df[\\'TIMESTAMP\\'] > TS_start ) & (df[\\'TIMESTAMP\\'] < TS_end)]\\n        l2 = len(df)\\n        dropped_rows=l1-l2\\n        masterrow[\\'starttime\\'] = df[\\'TIMESTAMP\\'].min()\\n        masterrow[\\'endtime\\'] = df[\\'TIMESTAMP\\'].max()\\n        masterrow[\\'dropped_headers\\']=dropped_headers\\n        masterrow[\\'dropped_rows\\']=dropped_rows\\n\\n        # Drop empty columns\\n        df = df.dropna(axis=1, how=\\'all\\')    \\n        masterrow[\\'calibrated\\'] = None\\n        for colnum, col in enumerate(df.columns):\\n            if col!=\\'TIMESTAMP\\':\\n                if \\'Unnamed\\' in col: \\n                    if dropped_headers: # if for TOB3 converted files, there is no column header on second row, we try the first row again DO NOT THINK THIS EVER HAPPENS\\n                        df.rename(columns=[col, columns_old[colnum]], inplace=True)\\n                        col = columns_old[colnum]\\n                    else:\\n                        continue\\n                        # just seems to be an index that was saved into CSV file in corrected directory.    print(\\'- Reverse calibration equations\\')\\n     \\n                try:\\n                    df[col]=df[col].astype(float)\\n                    if df[col].apply(float.is_integer).all():\\n                        df[col]=df[col].astype(int)\\n                        masterrow[col] = int(df[col].median())\\n                    else:\\n                        masterrow[col] = df[col].median()\\n                except Exception as e:\\n                    print(e)\\n                    print(f\\'Failed to convert column {col}\\')\\n                    print(df[col])\\n                    exit()\\n                if col[0:2]==\\'12\\' or col[0:2]==\\'21\\': # range seems to be 8000-10000 if not converted to calibrated units\\n                    if masterrow[col] > 7000.0:\\n                        masterrow[\\'calibrated\\'] = False\\n                    else:\\n                        masterrow[\\'calibrated\\'] = True\\n            #print(col, df[col].dtype)\\n        #mybasename = re.sub(f\\'masterrow[\"seqno\"]$\\', \\'\\', masterrow[\\'basename\\'])\\n        mybasename = masterrow[\\'basename\\'][:masterrow[\\'basename\\'].rfind(masterrow[\\'seqno\\'])]\\n        outdir = os.path.join(paths[\\'new_data\\'], masterrow[\\'uploadfolder\\'], masterrow[\\'sampratefolder\\'])\\n        outfile = mybasename + \\'_\\' +             masterrow[\\'starttime\\'].strftime(\\'%Y%m%d%H%M%S_\\') +             f\"{int(masterrow[\\'seqno\\']):03d}\" +             \\'.csv\\'\\n        outfullpath = os.path.join(outdir, outfile)\\n        if not os.path.isdir(outdir):\\n            os.makedirs(outdir)\\n        if masterrow[\\'calibrated\\']==True:\\n            outfullpath = outfullpath.replace(\\'.csv\\', \\'_REVERSED.csv\\')\\n        while os.path.isfile(outfullpath):\\n            outfullpath = outfullpath.replace(\\'.csv\\', \\'x.csv\\')\\n        if masterrow[\\'calibrated\\']==True:\\n            LLE.uncalibrate_to_raw(transducersDF, df, outfullpath)\\n        else:\\n            copy_to_raw(df, outfullpath)\\n        os.system(f\"echo {fullpath}, {outfullpath}, {masterrow[\\'basename\\']}, {masterrow[\\'starttime\\']}, {masterrow[\\'endtime\\']}, {masterrow[\\'seqno\\']} >> {FILELOOKUPCSV}\")\\n        \\n    return masterrow\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_tob3file(tob3file):\n",
    "    print(tob3file)\n",
    "    startdatetime=None\n",
    "    with open(tob3file, 'rb') as f:\n",
    "        firstline = f.readline().decode()\n",
    "    #print(firstline)\n",
    "    fields = firstline.split(',')\n",
    "    for field in fields:\n",
    "        if len(field)>5 and field[0:5]=='\\\"2022':\n",
    "            startdatetime = field[1:-2].replace('\\\"', '')\n",
    "    thisdict = {'file':os.path.basename(tob3file), 'datetime':startdatetime}\n",
    "    return thisdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lod = []\n",
    "tob3dir = paths['TOB3_DIR']\n",
    "for uploaddir in sorted(glob.glob(os.path.join(tob3dir, '202*'))):\n",
    "    for dirorfile in sorted(glob.glob(os.path.join(uploaddir, '*'))):\n",
    "        if os.path.isdir(dirorfile):\n",
    "            sampratedir = dirorfile\n",
    "            for tob3file in sorted(glob.glob(os.path.join(sampratedir, '*.dat'))):\n",
    "                thisdict = process_tob3file(tob3file)\n",
    "                thisdict['uploaddir']=os.path.basename(uploaddir)\n",
    "                thisdict['sampratedir']=os.path.basename(sampratedir)\n",
    "                lod.append(thisdict)\n",
    "\n",
    "        elif dirorfile[-4:] == '.dat':\n",
    "            sampratedir = ''\n",
    "            tob3file = dirorfile\n",
    "            thisdict = process_tob3file(tob3file)\n",
    "            thisdict['uploaddir']=os.path.basename(uploaddir)\n",
    "            thisdict['sampratedir']=os.path.basename(sampratedir)\n",
    "            lod.append(thisdict)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(lod)\n",
    "df.sort_values(by='datetime', inplace=True)\n",
    "df.to_csv('list_of_tob3_files_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
