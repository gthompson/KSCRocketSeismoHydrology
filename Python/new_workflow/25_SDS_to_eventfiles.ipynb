{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Segment SDS Archive based on launch event times and generate event web browser\n",
    "# \n",
    "# Launch times come from 'PilotStudy_KSC_Rocket_Launches.xlsx'\n",
    "# SDS archives are at SDS_TOP and contain data from wells 6I and 6S and from seismo-acoustic stations\n",
    "# Segmented event waveform files are saved as MiniSEED to EVENT_WAVEFORMS\n",
    "# \n",
    "# \n",
    "#from IPython import get_ipython\n",
    "#get_ipython().run_line_magic('run', 'header.ipynb')\n",
    "import os\n",
    "import header\n",
    "paths = header.setup_environment()\n",
    "paths['SDS_TOP'] = os.path.join(paths['outdir'], 'SDS')\n",
    "\n",
    "#import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy.core import read, Stream, UTCDateTime\n",
    "#import FDSNtools\n",
    "#import wrappers\n",
    "import SDS\n",
    "import Python.libWellData as LLE\n",
    "\n",
    "import libDatascopeGT\n",
    "from obspy.clients.filesystem.sds import Client\n",
    "\n",
    "HTML_DIR = '/var/www/html/thompsong/KSC_EROSION/EVENTS'\n",
    "PNG_DIR = os.path.join(HTML_DIR, 'images')\n",
    "EVENT_WAVEFORMS = os.path.join(paths['outdir'], 'EVENTS') # must exist, and Excel file must be here\n",
    "csv_launches = os.path.join(paths['outdir'], 'PilotStudy_KSC_Rocket_Launches.csv')\n",
    "csv_launches_detected = os.path.join(paths['outdir'], 'PilotStudy_KSC_Rocket_Launches_detected.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def try_different_waveform_loading_methods(thisSDSobj, startt, endt, methods=[1,2,3,4]):\n",
    "\n",
    "    # Datascope\n",
    "    dbpath = os.path.join(paths['outdir'], 'db', f'db{startt.strftime(\"%Y%m%d\")}' )\n",
    "    #st1 = libDatascopeGT.db2Stream(dbpath, startt, endt)\n",
    "\n",
    "    # Direct Miniseed load with ObsPy\n",
    "    st2 = libDatascopeGT.mseed2Stream(paths['SDS_TOP'], startt, endt)\n",
    "\n",
    "    # ObsPy SDS archive reader\n",
    "    sdsclient = Client(paths['SDS_TOP'])\n",
    "    st3 = sdsclient.get_waveforms(\"*\", \"*\", \"*\", \"[HDCES]*\", startt, endt)\n",
    "\n",
    "    # My SDS class that wraps ObsPy SDS reader\n",
    "    thisSDSobj.read(startt, endt, speed=1)\n",
    "    st4 = thisSDSobj.stream\n",
    "      \n",
    "    #compare_trace_ids(st1, st2, testA=1, testB=2)\n",
    "    #compare_trace_ids(st3, st2, testA=3, testB=2)\n",
    "    #compare_trace_ids(st4, st2, testA=4, testB=2) \n",
    "    #combine_streams(st2, st1)\n",
    "    combine_streams(st2, st3)    \n",
    "    combine_streams(st2, st4)\n",
    "    return st2\n",
    "    \n",
    "def combine_streams(stB, stA):\n",
    "    appended = False\n",
    "    for trA in stA:\n",
    "        found = False\n",
    "        for trB in stB:\n",
    "            if trA.stats.station == trB.stats.station and trA.stats.location == trB.stats.location and trA.stats.channel == trB.stats.channel:\n",
    "                if trA.stats.network == '':\n",
    "                    trA.stats.network = trB.stats.network\n",
    "                if trA.stats.starttime >= trB.stats.starttime and trA.stats.endtime <= trB.stats.endtime:\n",
    "                    found = True\n",
    "                    break\n",
    "        if not found:\n",
    "            stB.append(trA)\n",
    "            appended = True\n",
    "    if appended:\n",
    "        stB.merge(method=0, fill_value=0)\n",
    "\n",
    "\n",
    "\n",
    "def event_sds2pkl(launchtime, thisSDSobj, EVENT_WAVEFORMS, pretrig=3600, posttrig=3600, overwrite=False):    \n",
    "    rawfile = os.path.join(EVENT_WAVEFORMS, '%s_raw.pkl' % launchtime.strftime('%Y%m%dT%H%M%S'))\n",
    "    if os.path.exists(rawfile) and not overwrite:\n",
    "        print('%s already exists' % rawfile)\n",
    "    else:\n",
    "        print('segmenting %s from SDS' % rawfile) \n",
    "        startt = launchtime - pretrig\n",
    "        endt = launchtime + posttrig\n",
    "        st = try_different_waveform_loading_methods(thisSDSobj, startt, endt, methods=[1,2,3,4])\n",
    "\n",
    "        if len(st)>0:\n",
    "            try:\n",
    "                st.write(rawfile, format='pickle')\n",
    "            except:\n",
    "                st2 = Stream()\n",
    "                for tr in st:\n",
    "                    try:\n",
    "                        tr.write('tmp.pkl', 'pickle')\n",
    "                        st2.append(tr)\n",
    "                    except:\n",
    "                        print('Failed:\\n',tr)\n",
    "                st=st2\n",
    "                if len(st)>0:\n",
    "                    st.write(rawfile, format='pickle')\n",
    "                else:\n",
    "                    rawfile=None\n",
    "        else:\n",
    "            print('Got no data')\n",
    "            rawfile = None\n",
    "    if rawfile:\n",
    "        rawpng = os.path.join(PNG_DIR, os.path.basename(rawfile.replace('.pkl','.png')))\n",
    "        if not os.path.exists(rawpng):\n",
    "            print('creating raw file plot ',rawpng)\n",
    "            st.plot(equal_scale=False, outfile=rawpng)\n",
    "    return rawfile\n",
    "\n",
    "def clean(st):\n",
    "    for tr in st:\n",
    "        if tr.stats.network != 'FL':\n",
    "            continue\n",
    "        tr.detrend('linear')\n",
    "        tr.filter('highpass', freq=0.2, corners=2) \n",
    "        \n",
    "def apply_calibration_correction(st):\n",
    "    # calibration correction\n",
    "\n",
    "    for tr in st:\n",
    "        if 'countsPerUnit' in tr.stats:\n",
    "            continue\n",
    "        else:\n",
    "            tr.stats['countsPerUnit'] = 1\n",
    "            if not 'units' in tr.stats:\n",
    "                tr.stats['units'] = 'Counts'\n",
    "            if tr.stats.station[0].isnumeric(): # well data\n",
    "                if len(tr.stats.network)==0:\n",
    "                    tr.stats.network = '6'\n",
    "                if tr.stats.channel[2] == 'D':\n",
    "                    tr.stats.countsPerUnit = 1/LLE.psi2inches(1) # counts (psi) per inch\n",
    "                    tr.stats.units = 'inches'\n",
    "                elif tr.stats.channel[2] == 'H':\n",
    "                    tr.stats.countsPerUnit = 1/6894.76 # counts (psi) per Pa\n",
    "                    tr.stats.units = 'Pa'\n",
    "            elif tr.stats.channel[1]=='D':\n",
    "                tr.stats.countsPerUnit = 720 # counts/Pa on 1 V FS setting\n",
    "                if tr.id[:-1] == 'FL.BCHH3.10.HD':\n",
    "                    if tr.stats.starttime < UTCDateTime(2022,5,26): # Chaparral M25. I had it set to 1 V FS. Should have used 40 V FS. \n",
    "                        if tr.id == 'FL.BCHH3.10.HDF':\n",
    "                            tr.stats.countsPerUnit = 8e5 # counts/Pa\n",
    "                        else:\n",
    "                            tr.stats.countsPerUnit = 720 # counts/Pa \n",
    "                    else: # Chaparral switched to 40 V FS\n",
    "                        if tr.id == 'FL.BCHH3.10.HDF':\n",
    "                            tr.stats.countsPerUnit = 2e4 # counts/Pa\n",
    "                        else:\n",
    "                            tr.stats.countsPerUnit = 18 # counts/Pa \n",
    "                tr.stats.units = 'Pa'\n",
    "\n",
    "            elif tr.stats.channel[1]=='H':\n",
    "                tr.stats.countsPerUnit = 3e2 # counts/(um/s)\n",
    "                tr.stats.units = 'um/s'\n",
    "            tr.data = tr.data/tr.stats.countsPerUnit\n",
    "    \n",
    "def maxamp(tr):\n",
    "    return np.max(np.abs(tr.data))\n",
    "\n",
    "def remove_spikes(st):\n",
    "    SEISMIC_MAX = 0.1 # m/s\n",
    "    INFRASOUND_MAX = 3000 # Pa\n",
    "    FEET_MAX = 21 # feet\n",
    "    #SEISMIC_MIN = 1e-9\n",
    "    #INFRASOUND_MIN = 0.01\n",
    "    \n",
    "    for tr in st:\n",
    "        ma = maxamp(tr)\n",
    "        if tr.stats.units == 'm/s':\n",
    "            tr.data[tr.data > SEISMIC_MAX] = np.nan\n",
    "            tr.data[tr.data < -1 * SEISMIC_MAX] = np.nan             \n",
    "        elif tr.stats.units == 'Pa':\n",
    "            tr.data[tr.data > INFRASOUND_MAX] = np.nan\n",
    "            tr.data[tr.data < -1 * INFRASOUND_MAX] = np.nan   \n",
    "        elif tr.stats.units == 'feet':\n",
    "            tr.data[tr.data > FEET_MAX] = np.nan\n",
    "            tr.data[tr.data < -1 * FEET_MAX] = np.nan               \n",
    "\n",
    "from obspy.signal.trigger import coincidence_trigger\n",
    "from pprint import pprint\n",
    "import matplotlib.dates as dates\n",
    "def detectEvent(st, launchtime):\n",
    "    trig = coincidence_trigger(\"recstalta\", 3.5, 1, st, 3, sta=2, lta=40)\n",
    "    best_trig = {}\n",
    "    best_product = 0\n",
    "    for this_trig in trig:\n",
    "        thistime = dates.date2num(this_trig['time'])\n",
    "        this_product = this_trig['coincidence_sum']*this_trig['duration']\n",
    "        if this_product > best_product:\n",
    "            best_trig = this_trig\n",
    "            best_product = this_product\n",
    "    pprint(best_trig)\n",
    "    return best_trig['time']\n",
    "\n",
    "'''\n",
    "def add_snr(st, assoctime, threshold=1.5):\n",
    "    nstime = max([st[0].stats.starttime, assoctime-240])\n",
    "    netime = min([st[0].stats.endtime, assoctime-60])\n",
    "    sstime = assoctime\n",
    "    setime = min([st[0].stats.endtime, assoctime+120])    \n",
    "    for tr in st:\n",
    "        tr_noise = tr.copy().trim(starttime=nstime, endtime=netime)\n",
    "        tr_signal = tr.copy().trim(starttime=sstime, endtime=setime)\n",
    "        tr.stats['noise'] = np.nanmedian(np.abs(tr_noise.data))\n",
    "        tr.stats['signal'] = np.nanmedian(np.abs(tr_signal.data))\n",
    "        tr.stats['snr'] = tr.stats['signal']/tr.stats['noise']\n",
    "        '''\n",
    "\n",
    "def group_streams_for_plotting(st):\n",
    "    groups = {}\n",
    "    stationsWELL = ['6S', '6I']\n",
    "    for station in stationsWELL:\n",
    "        stationStream = st.select(network=station)\n",
    "        #stationIDS = list(set([tr.id for tr in stationStream]))\n",
    "        groups[station] = stationStream\n",
    "    streamSA = st.select(network='FL')\n",
    "    stationsSA = list(set([tr.stats.station for tr in streamSA]))\n",
    "    for station in stationsSA:\n",
    "        stationStream = streamSA.select(station=station)\n",
    "        #stationIDS = list(set([tr.id for tr in stationStream]))\n",
    "        groups[station] = stationStream\n",
    "    #print(groups)\n",
    "    return groups  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Read launch data into a DataFrame and generate a list of launch times in Python datetime.datetime format\n",
    "\n",
    "\n",
    "startover = True # starts with original CSV file again\n",
    "if os.path.isfile(csv_launches_detected) and startover==False:\n",
    "    launchesDF = LLE.removed_unnamed_columns(pd.read_csv(csv_launches_detected, index_col=None))\n",
    "else:\n",
    "    launchesDF = LLE.removed_unnamed_columns(pd.read_csv(csv_launches, index_col=None))\n",
    "    dt_tmp = pd.to_datetime(launchesDF['Date'] + ' ' +  launchesDF['Time'])\n",
    "    launchesDF['Date'] = [pdt.to_pydatetime() for pdt in dt_tmp]\n",
    "    launchesDF.drop(labels='Time', axis=1, inplace=True)\n",
    "    del dt_tmp\n",
    "\n",
    "for thisdir in [EVENT_WAVEFORMS, HTML_DIR, PNG_DIR]:\n",
    "    if not os.path.isdir(thisdir):\n",
    "        os.makedirs(thisdir)\n",
    "\n",
    "if not 'rawfile' in launchesDF.columns:\n",
    "    launchesDF['rawfile'] = ''\n",
    "if not 'corrected_file' in launchesDF.columns: \n",
    "    launchesDF['corrected_file'] = ''\n",
    "if not 'detection_time' in launchesDF.columns:\n",
    "    launchesDF['detection_time'] = '' \n",
    "if not 'short_file' in launchesDF.columns:\n",
    "    launchesDF['short_file'] = ''\n",
    "if not 'plotted' in launchesDF.columns:\n",
    "    launchesDF['plotted'] = False     \n",
    "launchesDF.to_csv(csv_launches_detected) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_st(st):\n",
    "    print(st)\n",
    "    wellst=st.select(network='6*')\n",
    "    if len(wellst)>0:\n",
    "        plot(equal_scale=False)\n",
    "        anykey = input('<Enter> to contine')\n",
    "\n",
    "# For each launch, segment raw SDS data to multi-trace MiniSEED file in EVENT_WAVEFORMS directory\n",
    "thisSDSobj = SDS.SDSobj(paths['SDS_TOP'])\n",
    "print(launchesDF)\n",
    "for i, row in launchesDF.iterrows():\n",
    "    launchTime = UTCDateTime(row['Date'])\n",
    "    if not row['corrected_file']:\n",
    "        print('Processing launch at %s' % launchTime.strftime('%Y-%m-%d %H:%M:%S')) \n",
    "        print(row)\n",
    "        if row['rawfile']:\n",
    "            rawfile = os.path.join(EVENT_WAVEFORMS, row['rawfile'])\n",
    "        else:\n",
    "            rawfile = event_sds2pkl(launchTime, thisSDSobj, EVENT_WAVEFORMS, overwrite=False)\n",
    "            if rawfile:\n",
    "                launchesDF.at[i, 'rawfile'] = os.path.basename(rawfile)\n",
    "            else:\n",
    "                raise Exception('failed to create %s' % rawfile)\n",
    "        try:\n",
    "            st = read(rawfile)    \n",
    "            #st.merge(method=0, fill_value=0)\n",
    "        except:\n",
    "            st = Stream()\n",
    "        print('%s: %d channels' % (rawfile,len(st)))\n",
    "\n",
    "        # all these functions safe for well traces too\n",
    "        print(st)\n",
    "        st.plot\n",
    "        clean(st) \n",
    "        print(st)\n",
    "        apply_calibration_correction(st)\n",
    "        print(st)\n",
    "        remove_spikes(st)\n",
    "        print(st)\n",
    "        \n",
    "        # write corrected event out\n",
    "        correctedfile =  os.path.join(EVENT_WAVEFORMS, '%s_long.pkl' % launchTime.strftime('%Y%m%dT%H%M%S'))\n",
    "        print('Writing %s' % correctedfile)\n",
    "        try:\n",
    "            st.write(correctedfile, format='PICKLE') # save 2-hour event waveforms\n",
    "            launchesDF.at[i, 'corrected_file'] = os.path.basename(correctedfile)\n",
    "        except:\n",
    "            pass\n",
    "del thisSDSobj\n",
    "launchesDF.to_csv(csv_launches_detected) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "           \n",
    "for i, row in launchesDF.iterrows():\n",
    "    if row['detection_time'] or not startover:\n",
    "        continue\n",
    "    launchTime = UTCDateTime(row['Date'])\n",
    "    if row['corrected_file']:\n",
    "        correctedfile =  os.path.join(EVENT_WAVEFORMS, row['corrected_file'])\n",
    "    else:\n",
    "        continue\n",
    "    print('Detecting launch at %s' % launchTime.strftime('%Y-%m-%d %H:%M:%S'))                        \n",
    "\n",
    "    # subset out the seismo-acoustic traces for detection purposes\n",
    "    if not os.path.isfile(correctedfile):\n",
    "        print('File not found: ',correctedfile)\n",
    "        continue\n",
    "    st = read(correctedfile)\n",
    "    SA = st.copy().select(network='FL').trim(starttime=launchTime-100, endtime=launchTime+200)\n",
    "    if len(SA)==0:\n",
    "        continue\n",
    "\n",
    "    assocTime = detectEvent(SA, launchTime)\n",
    "            \n",
    "    if abs(assocTime-launchTime)>100:\n",
    "        assocTime=launchTime\n",
    "    launchesDF.at[i, 'detection_time'] = assocTime\n",
    "launchesDF.to_csv(csv_launches_detected) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Short files\n",
    "for i, row in launchesDF.iterrows():                       \n",
    "    if not row['short_file']:\n",
    "        launchTime = UTCDateTime(row['Date']) \n",
    "        print('- Creating short file for launch at %s' % launchTime.strftime('%Y-%m-%d %H:%M:%S')) \n",
    "        if row['corrected_file']:\n",
    "            correctedfile =  os.path.join(EVENT_WAVEFORMS, row['corrected_file'])\n",
    "        else:\n",
    "            continue   \n",
    "        if not os.path.isfile(correctedfile):\n",
    "            print('File not found: ',correctedfile)\n",
    "            continue\n",
    "        # save 3-minute event waveforms\n",
    "        st = read(correctedfile)\n",
    "        if len(st)==0:\n",
    "            continue\n",
    "\n",
    "        assocTime = row['detection_time']\n",
    "        if not assocTime:\n",
    "            continue\n",
    "        st_short = st.copy()\n",
    "        #st_short.filter('highpass', freq=0.1, corners=2)\n",
    "        st_short.trim(starttime=assocTime-30, endtime=assocTime+150)\n",
    "        print(st_short)\n",
    "        if len(st_short)>0:\n",
    "            # write shorter corrected event out\n",
    "            shortfile =  os.path.join(EVENT_WAVEFORMS, '%s_short.pkl' % launchTime.strftime('%Y%m%dT%H%M%S'))\n",
    "            print('Writing %s' % shortfile)\n",
    "            try:\n",
    "                st_short.write(shortfile, format='PICKLE') # save 2-hour event waveforms\n",
    "            except:\n",
    "                pass\n",
    "            launchesDF.at[i, 'short_file'] = os.path.basename(shortfile)\n",
    "launchesDF.to_csv(csv_launches_detected)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plots\n",
    "for i, row in launchesDF.iterrows():\n",
    "    if row['plotted']:\n",
    "        continue\n",
    "    launchTime = UTCDateTime(row['Date'])\n",
    "    print('- Plotting launch at %s' % launchTime.strftime('%Y-%m-%d %H:%M:%S')) \n",
    "    for ext in ['long', 'short']:\n",
    "        if ext=='short':\n",
    "            pklfile = row['short_file']\n",
    "        else:    \n",
    "            pklfile = row['corrected_file']\n",
    "        if not pklfile:\n",
    "            continue\n",
    "        pklfile = os.path.join(EVENT_WAVEFORMS, pklfile)\n",
    "        if not os.path.isfile(pklfile):\n",
    "            print('File not found: ',pklfile)\n",
    "            continue\n",
    "        st = read(pklfile)\n",
    "        if len(st)==0:\n",
    "            continue        \n",
    "        groups = group_streams_for_plotting(st)\n",
    "        for station, stream_group in groups.items():\n",
    "            if len(stream_group)>0:\n",
    "                pngfile = os.path.join(PNG_DIR, '%s_%s_%s.png' % (launchTime.strftime('%Y%m%dT%H%M%S'), station, ext))\n",
    "                stream_group.plot(equal_scale=False, outfile=pngfile)\n",
    "    launchesDF.at[i, 'plotted'] = True\n",
    "launchesDF.to_csv(csv_launches_detected)   "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
