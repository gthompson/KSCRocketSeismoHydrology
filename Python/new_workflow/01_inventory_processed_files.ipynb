{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program Overview\n",
    "\n",
    "This program was used to convert data files in the Dropbox 2022_DATA directory (‘Dropbox/PROFESSIONAL/RESEARCH/3_Project_Documents/NASAprojects/201602_Rocket_Seismology/DATA/2022_DATA/WellData')\n",
    "\n",
    "What it does is:\n",
    "- loops over all directories\n",
    "- ignores the “combined” directory\n",
    "- loops over each filename ending with .csv or .pkl\n",
    "- ignores files with ‘gps’ in them – those were records of timing glitches\n",
    "- ignores files starting with “._” if the rest of the filename matches another that exists\n",
    "- checks if file already in reverse_lookup.csv, if not calls process_file, and appends to allmasterrows, which periodically is written to data_inventory.csv\n",
    "process_file() does the following:\n",
    "- splits dirname, filename to get uploadfolder, sampratefolder, basefilename, ext, realsamprate, seqno\n",
    "- loads filename\n",
    "- determines column headers (not always first row)\n",
    "- drops any rows with a TIMESTAMP more than 4 hours from the median TIMESTAMP\n",
    "- creates a row for the data_inventory.csv file with:\n",
    "    • - filename\n",
    "    • - topdir\n",
    "    • - uploadfolder\n",
    "    • - sampratefolder\n",
    "    • - basename\n",
    "    • - samprate\n",
    "    • - seqno\n",
    "    • - starttime\n",
    "    • - endtime\n",
    "    • - dropped_headers (boolean)\n",
    "    • - dropped_rows (number of)\n",
    "    • - calibrated (None=unknown, False, True)\n",
    "    • - median value for each transducer column\n",
    "- creates a file in /data/KSC/EROSION/fromdropboxinventory\n",
    "- creates a copy of the file,matching uploadfolder/sampratefolder/basename_starttime_seqno.csv, OR\n",
    "- if data calibrated, calls libWellData.py/uncalibrate_to_raw() to reverse the calibration, and replaces .csv with _REVERSED.csv\n",
    "\n",
    "# Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darwin\n",
      "HOME -> /Users/thompsong\n",
      "Developer -> /Users/thompsong/Developer\n",
      "repodir -> /Users/thompsong/Developer/KSCRocketSeismoHydrology/Python/new_workflow\n",
      "work -> /Users/thompsong/work\n",
      "local_outdir -> /Users/thompsong/work/PROJECTS/KSC_EROSION\n",
      "DROPBOX_TOP -> /Users/thompsong/Dropbox\n",
      "new_data -> /Users/thompsong/data/KSCwell\n",
      "DROPBOX_DATA_TOP -> /Users/thompsong/Dropbox/PROFESSIONAL/RESEARCH/3_Project_Documents/NASAprojects/201602_Rocket_Seismology/DATA/2022_DATA\n",
      "dropbox_outdir -> /Users/thompsong/Dropbox/PROFESSIONAL/RESEARCH/3_Project_Documents/NASAprojects/201602_Rocket_Seismology/DATA/2022_DATA/new_workflow\n",
      "WELLDATA_TOP -> /Users/thompsong/Dropbox/PROFESSIONAL/RESEARCH/3_Project_Documents/NASAprojects/201602_Rocket_Seismology/DATA/2022_DATA/WellData\n",
      "TOB3_DIR -> /Users/thompsong/Dropbox/PROFESSIONAL/RESEARCH/3_Project_Documents/NASAprojects/201602_Rocket_Seismology/DATA/2022_DATA/WellData/Uploads\n"
     ]
    }
   ],
   "source": [
    "import header\n",
    "paths = header.setup_environment()\n",
    "for k,v in paths.items():\n",
    "    print(k, '->', v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "#repopath = os.path.join(os.getenv('HOME'), 'Developer', 'KSCRocketSeismoHydrology')\n",
    "#os.chdir(repopath)\n",
    "#sys.path.append('Python')\n",
    "#print(os.getcwd())\n",
    "import glob\n",
    "import pandas as pd\n",
    "import obspy\n",
    "from IPython.display import clear_output\n",
    "FILELOOKUPCSV = os.path.join(paths['new_data'], 'reverse_lookup.csv')\n",
    "filesnotprocessedfile = os.path.join(paths['new_data'], 'files_not_processed.txt')\n",
    "MASTERCSV = os.path.join(paths['new_data'], f'data_inventory.csv')\n",
    "erase=False\n",
    "if erase:\n",
    "    os.system(f'rm -rf {paths['new_data']}/*')\n",
    "if not os.path.isfile(FILELOOKUPCSV):\n",
    "    os.system(f'echo \"fullpath, outpklfullpath, basename, starttime, endtime, seqno\" > {FILELOOKUPCSV}')\n",
    "lookupdf = pd.read_csv(FILELOOKUPCSV)\n",
    "if os.path.isfile(MASTERCSV):\n",
    "    allmasterrows = pd.read_csv(MASTERCSV).to_dict('records')\n",
    "else:\n",
    "    allmasterrows=[]\n",
    "    \n",
    "import libWellData as LLE\n",
    "transducersDF = pd.read_csv(paths['transducersCSVfile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def uncalibrate_to_raw(df, pklfile, to_csv=True):\n",
    "    print('- Reverse calibration equations')\n",
    "    for col in df.columns:\n",
    "        if col[0:2]=='12' or col[0:2]=='21':\n",
    "            this_transducer = transducersDF[(transducersDF['serial']) == col]\n",
    "            #print(this_transducer)\n",
    "            if len(this_transducer.index)==1:\n",
    "                this_transducer = this_transducer.iloc[0].to_dict()\n",
    "                #print(this_transducer)\n",
    "                df[col] = LLE.reverse_compute_psi(df[col].to_numpy(), this_transducer)\n",
    "    print('- writing reverse corrected data to %s' % pklfile)\n",
    "    if to_csv:       \n",
    "        df.to_csv(pklfile.replace('.pkl', '.csv'), index=False)\n",
    "    else:\n",
    "        df.to_pickle(pklfile)\n",
    "\"\"\"\n",
    "def copy_to_raw(df, pklfile, to_csv=True):\n",
    "    print('- copying data to %s' % pklfile)       \n",
    "    if to_csv:       \n",
    "        df.to_csv(pklfile.replace('.pkl', '.csv'), index=False)\n",
    "    else:\n",
    "        df.to_pickle(pklfile)\n",
    "\n",
    "def write_masterdf(allmasterrows):\n",
    "    if len(allmasterrows)>0:\n",
    "        masterdf = pd.DataFrame(allmasterrows)\n",
    "        print(f'Writing/updating {MASTERCSV}')\n",
    "        if 'starttime' in masterdf.columns:\n",
    "            masterdf.sort_values(by=['starttime','uploadfolder','sampratefolder','samprate']).to_csv(MASTERCSV, index=False)\n",
    "        else:\n",
    "            masterdf.to_csv(MASTERCSV, index=False)\n",
    "\n",
    "def process_file(dirpath, file, filenum, load=False):\n",
    "    clear_output()\n",
    "    print(f'Processing file {filenum}: {file} in {dirpath}')\n",
    "    fparts = file.split('.')\n",
    "    if len(fparts)==4:\n",
    "        uploadfolder, sampratefolder, basefilename, ext = fparts\n",
    "    elif len(fparts)==2:\n",
    "        uploadfolder = 'unknown'\n",
    "        sampratefolder = 'unknown'\n",
    "        basefilename, ext = fparts\n",
    "        dparts = dirpath.split('/')\n",
    "        if len(dparts)>3:\n",
    "            uploadfolder = dparts[-2]\n",
    "            sampratefolder = dparts[-1]\n",
    "    try:\n",
    "        sampratefolder1, middlename, realsamprateandseqno = basefilename.split('_')\n",
    "    except:\n",
    "        print('Failed to split: ',basefilename)\n",
    "        return 'failed to split basename'\n",
    "    if sampratefolder.lower() == sampratefolder1.lower(): # not a Baro file\n",
    "        if 'Hz' in realsamprateandseqno:\n",
    "            realsamprate, seqno = realsamprateandseqno.split('Hz')\n",
    "        elif 'Sec' in realsamprateandseqno:\n",
    "            realsamprate, seqno = realsamprateandseqno.split('Sec')\n",
    "        else:\n",
    "            print(f'Did not find Hz or Sec in filename: {file}')\n",
    "            return 'Did not find Hz or Sec in filename'\n",
    "    elif sampratefolder == 'Baro':\n",
    "        realsamprate, baro, seqno = basefilename.split('_')\n",
    "        realsamprate = realsamprate.split('hz')[0]\n",
    "        seqno = seqno.split('Sensors')[-1]\n",
    "    else:\n",
    "        print(f'samprate do not match: {file}, {sampratefolder}, {sampratefolder1}' +'\\n')\n",
    "        return 'samprate do not match'\n",
    "    #print(realsamprate, seqno)\n",
    "    #masterrow={'filename':os.path.basename(file), 'topdir':dirpath, 'uploadfolder':os.path.basename(uploadfolder), 'sampratefolder':sampratefolder, \\\n",
    "    #           'basename':basefilename, 'samprate':realsamprate, 'seqno':seqno} \n",
    "    masterrow={'filename':file, 'topdir':dirpath, 'uploadfolder':uploadfolder, 'sampratefolder':sampratefolder, \\\n",
    "               'basename':basefilename, 'samprate':realsamprate, 'seqno':seqno} \n",
    "\n",
    "\n",
    "    if load:\n",
    "        dropped_headers = False\n",
    "        dropped_rows = 0\n",
    "        fullpath = os.path.join(dirpath, file)\n",
    "        print(f'Loading {fullpath}')\n",
    "        try:\n",
    "            if file.endswith('pkl'):\n",
    "                df = pd.read_pickle(fullpath)\n",
    "            elif file.endswith('csv'):\n",
    "                df = pd.read_csv(fullpath)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            os.system(f'head {fullpath}')\n",
    "            raise e\n",
    "        \n",
    "        ''' Note that converted TOB3 files have multiple header lines and columns are read as dtype \"object\" because of mixed dtype. So we have to explicity convert them after removing excess header rows.\n",
    "            The first header row is garbage. We want the second '''\n",
    "\n",
    "\n",
    "        # Drop incorrect header row\n",
    "        columns_old = []\n",
    "        if not 'TIMESTAMP' in df.columns: # use 2nd row of file/0th row of dataframe for columns instead\n",
    "            columns_old = df.columns\n",
    "            df.columns = df.iloc[0]\n",
    "            df=df[1:]\n",
    "            dropped_headers=True\n",
    "            \n",
    "        # Convert TIMESTAMP\n",
    "        df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'], format='ISO8601', errors='coerce')\n",
    "        l1 = len(df)\n",
    "        df = df.dropna(subset=['TIMESTAMP'])\n",
    "\n",
    "        # filter by TIMESTAMP further\n",
    "        TS_median = df['TIMESTAMP'].median()\n",
    "        TS_start = TS_median - pd.Timedelta(hours=4)\n",
    "        TS_end = TS_median + pd.Timedelta(hours=4)\n",
    "        df = df[(df['TIMESTAMP'] > TS_start ) & (df['TIMESTAMP'] < TS_end)]\n",
    "        l2 = len(df)\n",
    "        dropped_rows=l1-l2\n",
    "        masterrow['starttime'] = df['TIMESTAMP'].min()\n",
    "        masterrow['endtime'] = df['TIMESTAMP'].max()\n",
    "        masterrow['dropped_headers']=dropped_headers\n",
    "        masterrow['dropped_rows']=dropped_rows\n",
    "\n",
    "        # Drop empty columns\n",
    "        df = df.dropna(axis=1, how='all')    \n",
    "        masterrow['calibrated'] = None\n",
    "        for colnum, col in enumerate(df.columns):\n",
    "            if col!='TIMESTAMP':\n",
    "                if 'Unnamed' in col: \n",
    "                    if dropped_headers: # if for TOB3 converted files, there is no column header on second row, we try the first row again DO NOT THINK THIS EVER HAPPENS\n",
    "                        df.rename(columns=[col, columns_old[colnum]], inplace=True)\n",
    "                        col = columns_old[colnum]\n",
    "                    else:\n",
    "                        continue\n",
    "                        # just seems to be an index that was saved into CSV file in corrected directory.    print('- Reverse calibration equations')\n",
    "     \n",
    "                try:\n",
    "                    df[col]=df[col].astype(float)\n",
    "                    if df[col].apply(float.is_integer).all():\n",
    "                        df[col]=df[col].astype(int)\n",
    "                        masterrow[col] = int(df[col].median())\n",
    "                    else:\n",
    "                        masterrow[col] = df[col].median()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(f'Failed to convert column {col}')\n",
    "                    print(df[col])\n",
    "                    exit()\n",
    "                if col[0:2]=='12' or col[0:2]=='21': # range seems to be 8000-10000 if not converted to calibrated units\n",
    "                    if masterrow[col] > 7000.0:\n",
    "                        masterrow['calibrated'] = False\n",
    "                    else:\n",
    "                        masterrow['calibrated'] = True\n",
    "            #print(col, df[col].dtype)\n",
    "        #mybasename = re.sub(f'masterrow[\"seqno\"]$', '', masterrow['basename'])\n",
    "        mybasename = masterrow['basename'][:masterrow['basename'].rfind(masterrow['seqno'])]\n",
    "        outdir = os.path.join(paths['new_data'], masterrow['uploadfolder'], masterrow['sampratefolder'])\n",
    "        outfile = mybasename + '_' + \\\n",
    "            masterrow['starttime'].strftime('%Y%m%d%H%M%S_') + \\\n",
    "            f\"{int(masterrow['seqno']):03d}\" + \\\n",
    "            '.csv'\n",
    "        outfullpath = os.path.join(outdir, outfile)\n",
    "        if not os.path.isdir(outdir):\n",
    "            os.makedirs(outdir)\n",
    "        if masterrow['calibrated']==True:\n",
    "            outfullpath = outfullpath.replace('.csv', '_REVERSED.csv')\n",
    "        while os.path.isfile(outfullpath):\n",
    "            outfullpath = outfullpath.replace('.csv', 'x.csv')\n",
    "        if masterrow['calibrated']==True:\n",
    "            LLE.uncalibrate_to_raw(transducersDF, df, outfullpath)\n",
    "        else:\n",
    "            copy_to_raw(df, outfullpath)\n",
    "        os.system(f\"echo {fullpath}, {outfullpath}, {masterrow['basename']}, {masterrow['starttime']}, {masterrow['endtime']}, {masterrow['seqno']} >> {FILELOOKUPCSV}\")\n",
    "        \n",
    "    return masterrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = os.getcwd()\n",
    "os.chdir(paths['WELLDATA_TOP'])\n",
    "print(os.listdir())\n",
    "if os.path.isfile(filesnotprocessedfile):\n",
    "    os.unlink(filesnotprocessedfile)\n",
    "masterdf=pd.DataFrame()\n",
    "\n",
    "filenum = 0\n",
    "for dirpath, dirnames, filenames in os.walk(\".\"):\n",
    "    if 'combined' in dirpath:\n",
    "        continue\n",
    "    for filename in sorted(filenames):\n",
    "        if filename.endswith((\".csv\", '*.pkl')):\n",
    "            masterrow = []\n",
    "            if 'gps' in filename or 'data_inventory' in filename or 'lookuptable' in filename or 'transducer' in filename or 'HOF' in filename or 'checkpoint' in filename:\n",
    "                masterrow = 'did not match file filter'\n",
    "            else:\n",
    "                if filename.startswith(\"._\"):\n",
    "                    filename2 = filename[2:]\n",
    "                    if filename2 in filenames:\n",
    "                        masterrow = 'starts with ._'\n",
    "            subsetdf = lookupdf[lookupdf['fullpath']==os.path.join(dirpath, filename)]\n",
    "            if len(subsetdf)>0:\n",
    "                masterrow='Processed already'\n",
    "\n",
    "            if not masterrow:\n",
    "                filenum += 1\n",
    "                print(filenum, dirpath, filename)\n",
    "                masterrow = process_file(dirpath, filename, filenum, load=True)\n",
    "            if isinstance(masterrow, dict):\n",
    "                allmasterrows.append(masterrow)\n",
    "                if filenum % 100 == 0:\n",
    "                    write_masterdf(allmasterrows)\n",
    "            else:\n",
    "                os.system(f\"echo {os.path.join(dirpath, filename)}: {masterrow} >> {filesnotprocessedfile}\")\n",
    "write_masterdf(allmasterrows)                \n",
    "os.chdir(pwd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
