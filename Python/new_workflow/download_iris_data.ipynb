{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d62b22",
   "metadata": {},
   "source": [
    "# Rocket Launch Seismic/Infrasound Data Download and Response Correction\n",
    "\n",
    "This notebook demonstrates how to download seismic and infrasound waveform data using ObsPy for all available stations within 200 miles of Kennedy Space Center during each rocket launch event provided in a CSV file. In addition, the notebook downloads instrument response information from the station inventory and removes the instrument response from the raw data.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **CSV Input:** The notebook reads a CSV file (using pandas) that contains details for each launch event. The CSV is expected to have the following columns:\n",
    "   - `name`: A unique event identifier\n",
    "   - `launch_time`: Launch date/time in ISO 8601 format (e.g., `2025-01-15T14:30:00`)\n",
    "   - `rocket_type`: The type of rocket (e.g., `Falcon 9`)\n",
    "   - `mission`: A brief mission description\n",
    "   - `payload`: Payload details\n",
    "   - `pre_event`: *(Optional)* Seconds before launch for the start of the data window (default: 1800 seconds)\n",
    "   - `post_event`: *(Optional)* Seconds after launch for the end of the data window (default: 7200 seconds)\n",
    "\n",
    "2. **Station Inventory:** The notebook uses ObsPy's FDSN client (using IRIS as an example) to query the station inventory (including instrument response information) for stations within approximately 200 miles (321 km) of Kennedy Space Center.\n",
    "\n",
    "3. **Waveform Download and Response Correction:** For each launch event, the notebook:\n",
    "   - Downloads the raw waveform data for all channels in the inventory over a time window defined by the event's launch time and pre/post-event durations.\n",
    "   - Saves the raw data as MiniSEED files.\n",
    "   - Removes the instrument response (using the downloaded inventory) with example pre-filter parameters and outputs the corrected data (e.g., ground velocity) to new MiniSEED files.\n",
    "\n",
    "Adjust the pre-filter parameters and output type based on your specific data and analysis requirements.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5a84077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Kennedy Space Center approximate coordinates\n",
    "KSC_LAT = 28.5729\n",
    "KSC_LON = -80.6490\n",
    "\n",
    "# 200 miles is roughly 321 km; convert km to degrees (1° ~ 111 km)\n",
    "radius_km = 321\n",
    "radius_deg = radius_km / 111.0  # ~2.89°\n",
    "\n",
    "# CSV file containing launch event details\n",
    "xls_file = '../../ksc_launch_events.xlsx'\n",
    "csv_file = 'ksc_launch_events_filtered.csv'\n",
    "\n",
    "# Default pre_event and post_event durations (in seconds)\n",
    "default_pre_event = 1800   # 30 minutes before launch\n",
    "default_post_event = 7200  # 2 hours after launch\n",
    "\n",
    "# Directory to store downloaded data\n",
    "data_dir = os.path.join(os.path.expanduser('~'), 'data', \"rocket_launch_data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Initialize ObsPy FDSN client (using IRIS as the example provider)\n",
    "client = Client(\"IRIS\")\n",
    "\n",
    "print(\"Configuration complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d25f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/ExtremeSSD1TB/Developer/GitHub/KSCRocketSeismoHydrology/Python/new_workflow\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing column provided to 'parse_dates': 'last_updated'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(csv_file):\n\u001b[0;32m----> 6\u001b[0m     launches_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwindow_start\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwindow_end\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlast_updated\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Volumes/ExtremeSSD1TB/Applications/anaconda3/envs/pygmt/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/ExtremeSSD1TB/Applications/anaconda3/envs/pygmt/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Volumes/ExtremeSSD1TB/Applications/anaconda3/envs/pygmt/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/ExtremeSSD1TB/Applications/anaconda3/envs/pygmt/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Volumes/ExtremeSSD1TB/Applications/anaconda3/envs/pygmt/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:161\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_usecols_names(\n\u001b[1;32m    156\u001b[0m             usecols,\n\u001b[1;32m    157\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    158\u001b[0m         )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_parse_dates_presence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_noconvert_columns()\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/ExtremeSSD1TB/Applications/anaconda3/envs/pygmt/lib/python3.10/site-packages/pandas/io/parsers/base_parser.py:243\u001b[0m, in \u001b[0;36mParserBase._validate_parse_dates_presence\u001b[0;34m(self, columns)\u001b[0m\n\u001b[1;32m    233\u001b[0m missing_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m    235\u001b[0m         {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     )\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_cols:\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing column provided to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_dates\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    245\u001b[0m     )\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Convert positions to actual column names\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    248\u001b[0m     col \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns) \u001b[38;5;28;01melse\u001b[39;00m columns[col]\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_needed\n\u001b[1;32m    250\u001b[0m ]\n",
      "\u001b[0;31mValueError\u001b[0m: Missing column provided to 'parse_dates': 'last_updated'"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Read Launch Event Data from CSV using pandas ---\n",
    "import json\n",
    "import pandas as pd\n",
    "print(os.getcwd())\n",
    "if os.path.isfile(csv_file):\n",
    "    launches_df = pd.read_csv(csv_file, index_col=None, parse_dates=['window_start', 'window_end'])\n",
    "else:\n",
    "    try:\n",
    "        launches_df = pd.read_excel(xls_file, index_col=None, parse_dates=['window_start', 'window_end'])\n",
    "        print(f\"Successfully read {len(launches_df)} launch events from {csv_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        raise\n",
    "    launches_df['SLC']=None\n",
    "    launches_df['success']=True\n",
    "    for i, row in launches_df.iterrows():\n",
    "        pad_json = row['pad'].replace(\"'s \",'s ').replace(\"'\", '\"').replace('True','true').replace('False','false').replace('None','null')\n",
    "        pad_dict = json.loads(pad_json)\n",
    "        pad_name = pad_dict[\"name\"]\n",
    "        launches_df.iat[i, launches_df.columns.get_loc('SLC')] = pad_name\n",
    "        if 'Failure' in row['status']:\n",
    "            launches_df.iat[i, launches_df.columns.get_loc('status')] = 'Failure'\n",
    "    launches_df = launches_df[['launch_designator', 'window_start', 'window_end', 'name', 'SLC', 'success'] ]\n",
    "    launches_df.sort_values(\"window_start\", inplace=True)\n",
    "    launches_df.to_csv(csv_file, index=False)\n",
    "launches_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c0bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Query Station Inventory (including instrument response) ---\n",
    "\n",
    "print(\"Querying station inventory within 200 miles of Kennedy Space Center...\")\n",
    "try:\n",
    "    inventory = client.get_stations(latitude=KSC_LAT, longitude=KSC_LON,\n",
    "                                    maxradius=radius_deg, level=\"channel\")\n",
    "    # Optionally, save inventory to file for inspection\n",
    "    inventory.write(\"stations.xml\", format=\"STATIONXML\")\n",
    "    network_count = len(inventory.get_contents().get(\"networks\", []))\n",
    "    print(f\"Found {network_count} networks.\")\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving station inventory:\", e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2420e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Process Each Rocket Launch Event ---\n",
    "\n",
    "for idx, row in launches_df.iterrows():\n",
    "    launch_name = row[\"name\"]\n",
    "    launch_time_str = row[\"launch_time\"]\n",
    "\n",
    "    try:\n",
    "        launch_time = UTCDateTime(launch_time_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing launch time for {launch_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Use provided pre_event and post_event if available; otherwise, defaults.\n",
    "    pre_event = int(row[\"pre_event\"]) if \"pre_event\" in row and not pd.isna(row[\"pre_event\"]) else default_pre_event\n",
    "    post_event = int(row[\"post_event\"]) if \"post_event\" in row and not pd.isna(row[\"post_event\"]) else default_post_event\n",
    "\n",
    "    start_time = launch_time - pre_event\n",
    "    end_time = launch_time + post_event\n",
    "\n",
    "    print(f\"\\nProcessing event: {launch_name}\")\n",
    "    print(f\"  Launch time: {launch_time}\")\n",
    "    print(f\"  Time window: {start_time} to {end_time}\")\n",
    "    print(f\"  Rocket Type: {row.get('rocket_type', '')}\")\n",
    "    print(f\"  Mission: {row.get('mission', '')}\")\n",
    "    print(f\"  Payload: {row.get('payload', '')}\")\n",
    "\n",
    "    # Create a directory for this launch event\n",
    "    event_dir = os.path.join(data_dir, launch_name)\n",
    "    os.makedirs(event_dir, exist_ok=True)\n",
    "\n",
    "    # Loop over each network, station, and channel in the inventory to download waveforms\n",
    "    for network in inventory:\n",
    "        for station in network:\n",
    "            station_code = station.code\n",
    "            for channel in station:\n",
    "                channel_code = channel.code\n",
    "                # Some channels may have an empty location code; use empty string if so.\n",
    "                location_code = channel.location_code if channel.location_code else \"\"\n",
    "                try:\n",
    "                    # Request waveform data for the specified time window\n",
    "                    st = client.get_waveforms(network.code, station_code, location_code, channel_code,\n",
    "                                              start_time, end_time)\n",
    "                    \n",
    "                    # Save the raw waveform data as MiniSEED\n",
    "                    raw_filename = f\"{network.code}.{station_code}.{location_code}.{channel_code}.{launch_name}.mseed\"\n",
    "                    raw_filepath = os.path.join(event_dir, raw_filename)\n",
    "                    st.write(raw_filepath, format=\"MSEED\")\n",
    "                    print(f\"  Downloaded raw data: {network.code}.{station_code}.{channel_code}\")\n",
    "                    \n",
    "                    # --- Instrument Response Removal ---\n",
    "                    try:\n",
    "                        # Detrend the data\n",
    "                        st.detrend(type=\"linear\")\n",
    "                        st.detrend(type=\"demean\")\n",
    "                        \n",
    "                        # Remove instrument response\n",
    "                        # Adjust the pre_filt parameters as needed\n",
    "                        st.remove_response(inventory=inventory,\n",
    "                                           output=\"VEL\",   # Change to \"DISP\" for displacement if needed\n",
    "                                           pre_filt=(0.01, 0.02, 8.0, 10.0),\n",
    "                                           water_level=60)\n",
    "                        \n",
    "                        corrected_filename = raw_filename.replace(\".mseed\", \".corr.mseed\")\n",
    "                        corrected_filepath = os.path.join(event_dir, corrected_filename)\n",
    "                        st.write(corrected_filepath, format=\"MSEED\")\n",
    "                        print(f\"  Instrument response removed: {network.code}.{station_code}.{channel_code}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Failed to remove instrument response for {network.code}.{station_code}.{channel_code} -- {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  Failed to download data for {network.code}.{station_code}.{channel_code} -- {e}\")\n",
    "\n",
    "print(\"\\nData download and instrument response correction complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
