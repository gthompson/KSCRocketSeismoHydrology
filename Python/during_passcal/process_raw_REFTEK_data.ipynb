{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRIS PASSCAL - RT130 Data Processing\n",
    "\n",
    "A Jupyter notebook by Glenn Thompson based on: https://www.passcal.nmt.edu/webfm_send/3035\n",
    "\n",
    "You’ve offloaded a service run and have data from each RT130. Follow the steps in this document to convert the data to miniSEED and reorganize it into station/channel/day volumes. Then, create a stationXML for your experiment using Nexus (see step 7) before submitting data to PASSCAL. Program names are in italics. Unix commands and any command line arguments are on separate lines. Input files are denoted by < filename>. Additional documentation can be found on the PASSCAL website: https://www.passcal.nmt.edu/content/passive-source-seed-archiving-documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed modules and set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1. Create an organized directory structure for your data. \n",
    "Start by creating a main directory for the project *(in this Jupyter Notebook, I use the variable REFTEKDIR for this main project directory)*. \n",
    "\n",
    "Under your main project directory, make a first level directory “SVC1” for service run number 1. For each subsequent service run create a new directory, e.g. SVC2, SVC3. Create directories in the SVC1 directory for the raw data files and log files. For example: \n",
    "\n",
    "    mkdir RAW\n",
    "    mkdir LOGS\n",
    "\n",
    "Move the raw data files (either .ZIP or CF folders) into the RAW directory, e.g.\n",
    "\n",
    "    mv SVC1/ZIPFILES/*.ZIP SVC1/RAW/\n",
    "\n",
    "<b>Glenn's variations:</b> While I sometimes used *Neo* to read the Compact Flash cards, compress the data to ZIP format, and then copy the data to the laptop, I mostly just copied the data using the MacOS command line as I found it quicker, e.g.\n",
    "\n",
    "    cp -r /Volumes/UNTITLED/RT130-*/2* SVC1/RAW/\n",
    "    \n",
    "At the time of writing this Jupyter Notebook, I had already completed the field project and had all the data organized into a directory structure that looks like:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SVC1/\n",
    "    RAW/\n",
    "        2018288/\n",
    "            AB13/\n",
    "                0/\n",
    "                1/\n",
    "                9/\n",
    "            9D7C/\n",
    "                0/\n",
    "                1/\n",
    "                9/\n",
    "            ...\n",
    "        2018289/\n",
    "                ...\n",
    "        ...  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b> The REFTEKDIR variable below must be set to the path of the parent directory of SVC1 (or SVC2, SVC3, etc.). If you set SERVICE_RUN = 1, the Jupyter Notebook will create the SVC1 directory underneath this (or SVC2 if you set it to 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REFTEKDIR = '/raid/data/KennedySpaceCenter/duringPASSCAL/REFTEK_DATA' \n",
    "REFTEKDIR = '/home/thompsong/work/PROJECTS/KSCpasscal'\n",
    "SERVICE_RUN = 0\n",
    "\n",
    "yn = input('Is %s path to the main project directory correct? ' % REFTEKDIR)\n",
    "if not (yn.lower() == 'yes' or yn.lower() == 'y'):\n",
    "    REFTEKDIR = input('Enter correct path to main project directory: ')\n",
    "    \n",
    "yn = input('Is this service run %d ? ' % SERVICE_RUN)\n",
    "if not (yn.lower() == 'yes' or yn.lower() == 'y'):\n",
    "    SERVICE_RUN = int(input('Enter correct service run number: '))\n",
    "       \n",
    "print('Setting paths for relative directories/files')\n",
    "SVCDIR = os.path.join(REFTEKDIR, 'SVC%d' % SERVICE_RUN)\n",
    "RAWDIR = os.path.join(SVCDIR, 'RAW') \n",
    "LOGSDIR =  os.path.join(SVCDIR, 'LOGS')\n",
    "CONFIGDIR = os.path.join(SVCDIR, 'CONFIG')\n",
    "MSEEDDIR = os.path.join(REFTEKDIR, 'MSEED')\n",
    "DAYSDIR = os.path.join(REFTEKDIR, 'DAYS')\n",
    "RT2MS_OUTPUT = os.path.join(LOGSDIR, 'rt2ms.out')\n",
    "\n",
    "print('Creating outline directory structure')\n",
    "if not os.path.exists(REFTEKDIR):\n",
    "    print(\"%s does not exist. Exiting\" % REFTEKDIR)\n",
    "    raise SystemExit(\"Killed!\")  \n",
    "subdirs = [SVCDIR, RAWDIR, LOGSDIR, CONFIGDIR, MSEEDDIR, DAYSDIR]\n",
    "for thissubdir in subdirs:\n",
    "    if not os.path.exists(thissubdir):\n",
    "        print('Need to make %s' % thissubdir)\n",
    "        os.mkdir(thissubdir)\n",
    "        if not os.path.exists(thissubdir):\n",
    "            print(\"%s does not exist & could not be created. Exiting\" % thissubdir)\n",
    "            raise SystemExit(\"Killed!\") \n",
    "if os.path.exists(RT2MS_OUTPUT):\n",
    "    os.remove(RT2MS_OUTPUT)\n",
    "print('Outline directory structure created/exists')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse RT130 9/* files to reconstruct digitizer-station history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary9file = '/media/sda1/summary9file.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATIONS = ['BHP', 'TANK', 'FIRE', 'BCHH', 'DVEL', 'RBLAB']\n",
    "dayfullpaths = sorted(glob.glob('%s/20?????' % RAWDIR))\n",
    "if os.path.exists(summary9file):\n",
    "    os.remove(summary9file)\n",
    "count = 0\n",
    "filehandle = open(summary9file, \"w\")\n",
    "for thisdayfullpath in dayfullpaths:\n",
    "    count = count + 1\n",
    "    print('Processing %s (%d of %d)' % (thisdayfullpath, count, len(dayfullpaths) ), end=\"\\r\", flush=True)\n",
    "    thisdaydir = os.path.basename(thisdayfullpath) # a directory like 2018365\n",
    "    rt130files = sorted(glob.glob('%s/????/9/*' % thisdayfullpath))\n",
    "    if rt130files:\n",
    "        \n",
    "        for rt130file in rt130files:\n",
    "            output = os.popen('strings %s' % rt130file).read()\n",
    "            if output: \n",
    "                for station in STATIONS:\n",
    "                    firstindex = output.find(station)\n",
    "                    if firstindex > -1:\n",
    "                        break\n",
    "                    \n",
    "                if firstindex != -1:\n",
    "                    pathparts = rt130file.split('/')\n",
    "                    rt130 = pathparts[-3]\n",
    "                    filehandle.write('%s, %s, %s\\n' % (output[firstindex:firstindex+4], thisdaydir, rt130) )\n",
    "         \n",
    "        #if count>40:\n",
    "        #    break\n",
    "        \n",
    "filehandle.close()\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = os.popen('sort %s | uniq' % summary9file).read()\n",
    "#print(output)\n",
    "lastStationDigitizerCombo = \"\"\n",
    "lastline = \"\"\n",
    "\n",
    "for thisline in output.split('\\n'):\n",
    "    try:\n",
    "        (station, yyyyjjj, digitizer)  = thisline.split(',')\n",
    "        thisStationDigitizerCombo = '%s%s' % (station, digitizer)\n",
    "        #print(lastStationDigitizerCombo, thisStationDigitizerCombo)\n",
    "        if lastStationDigitizerCombo != thisStationDigitizerCombo:\n",
    "            print(lastline)\n",
    "            print(thisline)\n",
    "        lastStationDigitizerCombo = thisStationDigitizerCombo\n",
    "        lastline = thisline\n",
    "    except:\n",
    "        pass\n",
    "print(lastline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse RT130 1/* files to reconstruct digitizer-GPS_Position history\n",
    "This generates a DataFrame/CSV file like:\n",
    "\n",
    "<pre>\n",
    ",Digitizer,yyyyjjj,Latitude,LatSTD\n",
    "0,92B7,2018222,28.0595,2.8867513458681453e-05\n",
    "1,92B7,2018223,28.0595,1.8559214542252517e-05\n",
    "2,92B7,2018229,28.573463888888888,2.721655270429592e-06\n",
    "3,92B7,2018230,28.573469444444445,1.7899429988238652e-06\n",
    "4,92B7,2018231,28.573469444444445,2.6032870393506316e-06\n",
    "5,92B7,2018233,28.573469444444445,3.552713678800501e-15\n",
    "6,92B7,2018234,28.573469444444445,3.552713678800501e-15\n",
    "7,92B7,2018235,28.573469444444445,3.552713678800501e-15\n",
    "</pre>\n",
    "\n",
    "The next task will be to detect dates on which a digitizer was moved by examining plots of GPS Latitude & standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all(a_str, sub):\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = a_str.find(sub, start)\n",
    "        if start == -1: return\n",
    "        yield start\n",
    "        start += len(sub) # use start += 1 to find overlapping matches\n",
    "\n",
    "#list(find_all('spam spam spam spam', 'spam'))\n",
    "\n",
    "positionDF = pd.DataFrame(columns = ['Digitizer', 'yyyyjjj', 'Latitude', 'LatSTD'])\n",
    "digitizerList = list()\n",
    "yyyyjjjList = list()\n",
    "latitudeList = list()\n",
    "latSTDList = list()\n",
    "dayfullpaths = sorted(glob.glob('%s/20?????' % RAWDIR))\n",
    "summary1file = '/media/sda1/summary1file.csv'\n",
    "if os.path.exists(summary1file):\n",
    "    os.remove(summary1file)\n",
    "count = 0\n",
    "#filehandle = open(summary1file, \"w\")\n",
    "for thisdayfullpath in dayfullpaths:\n",
    "    count = count + 1\n",
    "    #print('Processing %s (%d of %d)' % (thisdayfullpath, count, len(dayfullpaths) ), end=\"\\r\", flush=True)\n",
    "    thisdaydir = os.path.basename(thisdayfullpath) # a directory like 2018365\n",
    "    digitizerpaths = sorted(glob.glob('%s/????' % thisdayfullpath))\n",
    "    for digitizerpath in digitizerpaths:\n",
    "        thisdigitizer = os.path.basename(digitizerpath)\n",
    "        rt130files = sorted(glob.glob('%s/1/*' % digitizerpath))\n",
    "        if rt130files:\n",
    "            all_positions = list()\n",
    "            for rt130file in rt130files:\n",
    "                print('Processing %s' % rt130file, end=\"\\r\", flush=True)\n",
    "                \n",
    "                output = os.popen('strings %s' % rt130file).read()\n",
    "                if output: \n",
    "                    \n",
    "                    all_indexes = list(find_all(output, 'POSITION'))\n",
    "                \n",
    "                    for each_index in all_indexes:\n",
    "                        \n",
    "                        Ndeg = output[each_index+11:each_index+13]\n",
    "                        Nmin = output[each_index+14:each_index+16]\n",
    "                        Nsec = output[each_index+17:each_index+22]\n",
    "                        Ndecdeg = float(Ndeg) + float(Nmin)/60 + float(Nsec)/3600\n",
    "                        \n",
    "                        if Ndecdeg > 28.0 and Ndecdeg < 29.0:\n",
    "                            #print(output[each_index+11:each_index+22], \" \", Ndecdeg)\n",
    "                            all_positions.append(Ndecdeg)\n",
    "            if all_positions:  \n",
    "                all_positions_np = np.array(all_positions)\n",
    "                #print(all_positions_np)\n",
    "                #print(np.median(all_positions_np))\n",
    "                #print('%s, %s, %f, %f\\n' % (thisdigitizer, thisdaydir, np.median(all_positions_np), np.std(all_positions_np)) )\n",
    "                #filehandle.write('%s, %s, %f\\n' % (thisdigitizer, thisdaydir, np.median(all_positions_np)  ) )\n",
    "                digitizerList.append(thisdigitizer)\n",
    "                yyyyjjjList.append(thisdaydir)\n",
    "                latitudeList.append(np.median(all_positions_np))\n",
    "                latSTDList.append(np.std(all_positions_np))\n",
    "         \n",
    "    #if count>20:\n",
    "    #    break\n",
    "\n",
    "        \n",
    "#filehandle.close()\n",
    "positionDF['Digitizer'] = digitizerList\n",
    "positionDF['yyyyjjj'] = yyyyjjjList\n",
    "positionDF['Latitude'] = latitudeList\n",
    "positionDF['LatSTD'] = latSTDList\n",
    "\n",
    "positionDF.to_csv(summary1file)\n",
    "\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert yyyyjjj strings to datetime objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#print(positionDF)\n",
    "dates = list()\n",
    "for thisyyyyjjj in positionDF['yyyyjjj']:\n",
    "    thisdate = datetime.datetime.strptime(thisyyyyjjj, \"%Y%j\").date()\n",
    "    dates.append(thisdate)\n",
    "#print(dates)\n",
    "positionDF['dates']=dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.formatter.useoffset'] = False # do not allow relative y-labels\n",
    "\n",
    "allDigitizers = list(set(digitizerList))\n",
    "for digitizer in allDigitizers:\n",
    "    subsetDF = positionDF[positionDF['Digitizer']==digitizer]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot_date(subsetDF['dates'], subsetDF['Latitude'],'*')  \n",
    "    subsetDF.to_csv('latitude_' + digitizer + '.csv')\n",
    "    ax.set_title(digitizer)\n",
    "    if not (digitizer == 'AB13' or digitizer == '9D7C'):\n",
    "        ax.set_ylim(28.572, 28.575)\n",
    "    #xt = ax.get_xticks()\n",
    "    #ax.set_xticks(xt[0::30])\n",
    "    for xtl in ax.get_xticklabels():\n",
    "        xtl.set_rotation(30)\n",
    "        xtl.set_horizontalalignment('right')\n",
    "    #plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "allDigitizers = list(set(digitizerList))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for digitizer in allDigitizers:\n",
    "    subsetDF = positionDF[positionDF['Digitizer']==digitizer]\n",
    "    ax.plot_date(subsetDF['dates'], subsetDF['Latitude'],'.', label=digitizer)  \n",
    "    \n",
    "#xt = ax.get_xticks()\n",
    "#ax.set_xticks()\n",
    "ax.set_xlim(dates[0],dates[-1])\n",
    "for xtl in ax.get_xticklabels():\n",
    "    xtl.set_rotation(30)\n",
    "    xtl.set_horizontalalignment('right')\n",
    "plt.legend()\n",
    "ax.set_ylim(28.51, 28.58)\n",
    "plt.savefig('digitizer_lats.png')\n",
    "ax.set_ylim(28.572, 28.575)\n",
    "plt.savefig('digitizer_lats_zoomed.png')\n",
    "#plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2. Create the parameter file(s). \n",
    "The parameter file is used by *rt2ms* to assign header information to the miniSEED files. *rt2ms* is a PASSCAL program that gene\n",
    "rates miniSEED formatted files from REFTEK RT130 raw files. In addition, *rt2ms* also modifies the headers. In the SVC1 directory, use a text editor and information from your field notes to create an ASCII parameter file (parfile) following the examples at https://www.passcal.nmt.edu/webfm_send/3035.\n",
    "\n",
    "<b>Glenn's variations</b>: PASSCAL instructions assume you construct a par file by hand for each network layout. However, I construct an Antelope-style *dbbuild_batch* pf file by hand for each network layout, and then use the PASSOFT program *batch2par* to convert this to a par file (in combination with 2 *sed* (Unix stream editor) commands to fix this. You can see *batch2par* and *sed* commands used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('For each dbbuild_batch pf file, create a corresponding par file')\n",
    "pffiles = sorted(glob.glob('%s/network*.pf' % CONFIGDIR))\n",
    "for pffile in pffiles:\n",
    "    parfile = pffile[:-2] + 'par' # \n",
    "    print('- %s, %s' % (pffile, parfile)) \n",
    "            \n",
    "    # Create the corresponding parfile if it does not already exist\n",
    "    if not os.path.exists(parfile): \n",
    "        if commandExists('batch2par'): \n",
    "            os.system(\"batch2par %s -m > %s\" % (pffile, parfile))\n",
    "            if os.path.exists(parfile): \n",
    "                # Edit the par file\n",
    "                os.system(\"sed -i -e 's/rs200spsrs;/1;         /g' %s\" % parfile)\n",
    "                os.system(\"sed -i -e 's/x1/32/g' %s\" % parfile);  \n",
    "            else:\n",
    "                print(\"- batch2par failed\")\n",
    "                raise SystemExit(\"Killed!\")\n",
    "        else:\n",
    "            raise SystemExit(\"Killed!\")\n",
    "print('All pf files now have a corresponding par file')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions for part 3\n",
    "\n",
    "*chooseCorrectParFile* selects the par file with YYYYJJJ closest to, but not exceeding, the day of Reftek data we are trying to process. For example, for 2018200 (200th day of year 2018), we match network2018200.par if it exists. Otherwise, network2018199.par is the best match, if it exists. Or whatever the closest pf is *before* the data day. But network2018201.par is not a match, since the existence of that pf would indicate the network changed on that day, so it was no longer applicable to 2018200 data.\n",
    "\n",
    "*commandExists* checks if PASSOFT/DMC commands are installed before we try to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseCorrectParFile(yyyyjjj):\n",
    "    correctparfile = \"\"\n",
    "    parfilelist = sorted(glob.glob('%s/network20?????.par' % CONFIGDIR))\n",
    "    for thisparfile in parfilelist:\n",
    "        parfileyyyyjjj = thisparfile[-11:-4]\n",
    "        #print(parfileyyyyjjj, yyyyjjj)\n",
    "        if parfileyyyyjjj <= yyyyjjj:\n",
    "            correctparfile = thisparfile\n",
    "    return correctparfile\n",
    "\n",
    "def commandExists(command):\n",
    "    output = os.popen('which %s' % command).read()\n",
    "    if output:\n",
    "        return True\n",
    "    else:\n",
    "        print('Command %s not found.' % command)\n",
    "        print('Make sure the PASSOFT tools are installed on this computer, and available on the $PATH')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Convert your data into miniSEED files. \n",
    "In the service run directory, convert the raw RT130 data to miniSEED. Typing *rt2ms -h* shows a list of available options. \n",
    "\n",
    "If raw data is in decompressed folders, use the following commands: \n",
    "\n",
    "    ls -d SVC1/RAW/*.cf > file.lst rt2ms -F file.lst -Y -L -o MSEED/ -p <parfile> >& rt2ms.out \n",
    "\n",
    "The (-F) flag will process all files in the named list, (‐Y) puts the data in yearly directories, (-L) outputs .log and, if created, .err files, (‐o) creates an output directory, MSEED, and (‐p) points to your parfile. \n",
    "\n",
    "If raw data is in ZIP files: \n",
    "\n",
    "    rt2ms ‐D SVC1/RAW/ ‐Y ‐L -o MSEED/ ‐p <parfile> >& rt2ms.out \n",
    "\n",
    "The (‐D) flag will process all .ZIP files in a specified directory, instead of in a file list as in the previous example. \n",
    "\n",
    "When *rt2ms* finishes, move all of your log and .err files from the MSEED directory to the LOGS directory that you created in step 1. \n",
    "\n",
    "After running *rt2ms* the MSEED directory structure should look something like the example below. In the MSEED directory there will be .log files and possibly .err files along with a  subdirectory for each year that contains day directories for each stream."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MSEED/\n",
    "    2014.019.21.29.16.98EZ.log\n",
    "    2014.019.21.29.16.98EZ.err\n",
    "    Y2014/\n",
    "        R065.01/\n",
    "        R065.02/\n",
    "        R065.03/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dayfullpaths = sorted(glob.glob('%s/20?????' % RAWDIR))\n",
    "for thisdayfullpath in dayfullpaths:\n",
    "    print('Processing %s' % thisdayfullpath)\n",
    "    thisdaydir = os.path.basename(thisdayfullpath) # a directory like 2018365\n",
    "    \n",
    "    # Find the corresponding parameter file for this day of the experiment\n",
    "    parfile = chooseCorrectParFile(thisdaydir)\n",
    "    if os.path.exists(parfile):                \n",
    "        # Par file must exist if we got here. Run rt2ms.\n",
    "        # Here we would ideally check that we have a full set of corresponding 0/, 1/ and 9/ files,\n",
    "        # and we would also check if the hourly MSEED file already exists, and only run this if it does not\n",
    "        if commandExists('rt2ms'): \n",
    "            tmplistfile = 'tmpfilelist.txt'\n",
    "            rt130files = sorted(glob.glob('%s/????/[19]/*' % thisdayfullpath))\n",
    "            if rt130files:\n",
    "                with open(tmplistfile, 'w') as filehandle:\n",
    "                    #for rt130file in rt130files:\n",
    "                    #    filehandle.write('%s\\n' % rt130file)\n",
    "                    filehandle.writelines(\"%s\\n\" % listitem for listitem in rt130files)    \n",
    "                #os.system('cat %s' % tmplistfile)\n",
    "                #input('<ENTER> to continue')\n",
    "                #os.system('ls %s/????/1/* > %s' % (thisdayfullpath, tmplistfile) )\n",
    "                #os.system('ls %s/????/9/* >> %s' % (thisdayfullpath, tmplistfile) )\n",
    "\n",
    "\n",
    "                # We can check the following RT2MS_OUTFILE if rt2ms fails\n",
    "                os.system(\"rt2ms -F %s -Y -L -v -p %s -o %s >> %s\" % (tmplistfile, parfile, MSEEDDIR, RT2MS_OUTPUT))\n",
    "                os.remove(tmplistfile) # remove it here so we can never get the wrong one\n",
    "\n",
    "                # move all *.log files to the LOGS directory\n",
    "                for src_file in Path(MSEEDDIR).glob('*.log'):\n",
    "                    shutil.copy(src_file, LOGSDIR)\n",
    "\n",
    "                # move all *.err files to the LOGS directory\n",
    "                for src_file in Path(MSEEDDIR).glob('*.err'):\n",
    "                    shutil.copy(src_file, LOGSDIR)         \n",
    "                \n",
    "        else:\n",
    "            raise SystemExit(\"Killed!\")\n",
    "        \n",
    "    else:\n",
    "        print('- no corresponding parameter file found')\n",
    "        \n",
    "if dayfullpaths:\n",
    "    print('FINISHED CONVERTING REFTEK DATA TO MINISEED HOURLY FILES')\n",
    "else:\n",
    "    print('No directories like RAW/YYYYJJJ found')\n",
    "print('Done')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Reorganize the miniSEED data into station/channel/day volumes.\n",
    "\n",
    "*dataselect* is a DMC program that allows for the extracting and sorting of miniSEED data (https://github.com/iris-edu/dataselect). This will read the data from the MSEED directory and convert them into day volumes with the required naming format: \n",
    "\n",
    "    dataselect -A DAYS/%s/%s.%n.%l.%c.%Y.%j MSEED/Y*/*/* \n",
    "\n",
    "The (-A) flag writes file names in the specified custom format. The format flags are (s) for station, (n) for netcode, (l) for location, (c) for channel name, (Y) for year, and (j) for Julian date. See the help menu for more details on options (*dataselect -h*). Depending on how much data you have, you may need to run *dataselect* in a loop that runs over the different days or stations in your experiment.\n",
    "\n",
    "<b>Please note:</b> PASSCAL want data to be organized in BUD format. *dataselect -h* reveals that there is a BUD format option built in directly, so I attempt to use that here instead. This command is easier: \n",
    "\n",
    "    dataselect -BUD MSEED/Y*/*/* \n",
    "    \n",
    "However, I do loop over year and day directories (as suggested) so that *dataselect* is not trying to deal with a file list that is too long for it or the operating system to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if commandExists('dataselect'): \n",
    "    # SCAFFOLD. I need to install this from github. And then figure out how to\n",
    "    # substitute for station, netcode, location, channel name, year and julian day\n",
    "    # May need to lop over different stations (I already do by day)\n",
    "    mseedyearfullpaths = sorted(glob.glob('%s/Y20??' % MSEEDDIR))\n",
    "    for thismseedyearfullpath in mseedyearfullpaths:\n",
    "        print('Processing %s' % thismseedyearfullpath)  \n",
    "        mseeddayfullpaths = sorted(glob.glob('%s/R*.01' % thismseedyearfullpath))\n",
    "        for thismseeddayfullpath in mseeddayfullpaths:\n",
    "            print('dataselect: Processing %s' % thismseeddayfullpath)\n",
    "            #os.system('dataselect  -A %s/%s/%s.%n.%l.%c.%Y.%j  %s/*.m' % (DAYSDIR, thismseeddayfullpath))\n",
    "            os.system('dataselect -v -BUD %s %s/*.m' % (DAYSDIR, thismseeddayfullpath) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: Confirm your station and channel names\n",
    "In the DAYS folder just created by dataselect, check to see if you have folders for each of your stations. The data should be organized into those folders in station/channel/day volumes named STA.NET.LOC.CHAN.YEAR.JULDAY. For example: BA01.XR..HHZ.2018.039 (The .. after XR is where the location code would be if needed).\n",
    "\n",
    "If your parfile was incomplete (i.e. missing stations or channels), there will be one or more folders named with the RT130 serial numbers (e.g. 9306) instead of the desired station name (e.g. ME42). To change any miniSEED headers to correct a station name, network code, etc., see the *fixhdr* doc on the PASSCAL website (see link on the first page). After you have modified the headers with *fixhdr*, rename the files so that the station‐network‐location‐channel codes in the miniSEED file names match the corrected headers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6. Perform quality control of waveforms and logs. \n",
    "Verify the data quality by reviewing the traces and log files (with *logpeek* and *pql*). Obvious signs of trouble include loss of GPS timing, overlaps, gaps, corrupted files, etc. Make a note of any problems. Use *fixhdr* to correct mark timing issues, and/or to convert the files to big endianess if they are not already. For more information on how to use these tools, refer to the appropriate documentation on the PASSCAL website (see link on the first page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7. Create metadata for your experiment. \n",
    "Use *Nexus* to generate a stationXML file for your experiment metadata. See the “Metadata Generation with Nexus in a Nutshell” document on the PASSCAL website (see link on first page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8. Send miniSEED data to PASSCAL. \n",
    "Please drop a note, with your PASSCAL project name in the subject, to <mailto>data_group@passcal.nmt.edu</mailto> before sending the data to PASSCAL so that we can set up a receiving area. Attach the stationXML created with Nexus to this email unless it is larger than 5Mb. Use our tool *data2passcal* to send the data: \n",
    "\n",
    "    data2passcal DAYS/ \n",
    "\n",
    "*data2passcal* will scan all subdirectories of the DAYS folder and send any miniSEED files that have the correct file names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
