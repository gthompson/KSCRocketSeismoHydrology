{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRIS PASSCAL - RT130 Data Processing: Step 2 - Generating parameter files\n",
    "\n",
    "A Jupyter notebook by Glenn Thompson based on: https://www.passcal.nmt.edu/webfm_send/3035\n",
    "\n",
    "You’ve offloaded a service run and have data from each RT130. Follow the steps in this document to convert the data to miniSEED and reorganize it into station/channel/day volumes. Then, create a stationXML for your experiment using Nexus (see step 7) before submitting data to PASSCAL. Program names are in italics. Unix commands and any command line arguments are on separate lines. Input files are denoted by < filename>. Additional documentation can be found on the PASSCAL website: https://www.passcal.nmt.edu/content/passive-source-seed-archiving-documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed modules and set global variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1. Create an organized directory structure for your data. \n",
    "Start by creating a main directory for the project *(in this Jupyter Notebook, I use the variable REFTEKDIR for this main project directory)*. \n",
    "\n",
    "Under your main project directory, make a first level directory “SVC1” for service run number 1. For each subsequent service run create a new directory, e.g. SVC2, SVC3. Create directories in the SVC1 directory for the raw data files and log files. For example: \n",
    "\n",
    "    mkdir RAW\n",
    "    mkdir LOGS\n",
    "\n",
    "Move the raw data files (either .ZIP or CF folders) into the RAW directory, e.g.\n",
    "\n",
    "    mv SVC1/ZIPFILES/*.ZIP SVC1/RAW/\n",
    "\n",
    "<b>Glenn's variations:</b> While I sometimes used *Neo* to read the Compact Flash cards, compress the data to ZIP format, and then copy the data to the laptop, I mostly just copied the data using the MacOS command line as I found it quicker, e.g.\n",
    "\n",
    "    cp -r /Volumes/UNTITLED/RT130-*/2* SVC1/RAW/\n",
    "    \n",
    "At the time of writing this Jupyter Notebook, I had already completed the field project and had all the data organized into a directory structure that looks like:\n",
    "\n",
    "<pre>\n",
    "    SVC1/\n",
    "    RAW/\n",
    "        2018288/\n",
    "            AB13/\n",
    "                0/\n",
    "                1/\n",
    "                9/\n",
    "            9D7C/\n",
    "                0/\n",
    "                1/\n",
    "                9/\n",
    "            ...\n",
    "        2018289/\n",
    "                ...\n",
    "        ...  \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import obspy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.formatter.useoffset'] = False # do not allow relative y-labels\n",
    "\n",
    "#############################\n",
    "####  STEP ONE FUNCTIONS ####\n",
    "#############################\n",
    "def find_all(a_str, sub):\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = a_str.find(sub, start)\n",
    "        if start == -1: return\n",
    "        yield start\n",
    "        start += len(sub) # use start += 1 to find overlapping matches\n",
    "\n",
    "def read_rt130_1_file(rt130file):\n",
    "    #print('Processing %s' % rt130file, end=\"\\r\", flush=True)\n",
    "    #print(rt130file)\n",
    "    all_lats = [] \n",
    "    londeg = None\n",
    "    latdeg = None\n",
    "    output = os.popen('strings %s' % rt130file).read()\n",
    "    if output: \n",
    "\n",
    "        #print(type(output))\n",
    "        lines = output.split('\\n')\n",
    "        #print(len(lines))\n",
    "        '''for i,line in enumerate(lines[0:3]):\n",
    "            print(f\"[{i}], {line}, [length={len(line)}]\")\n",
    "        '''\n",
    "\n",
    "        lineindex = 1\n",
    "        latindex = lines[lineindex].find('YABBBBBC        N ')\n",
    "        if latindex == -1:\n",
    "            for i, line in enumerate(lines):\n",
    "                latindex = line.find('YABBBBBC        N ')\n",
    "                if latindex > -1:\n",
    "                    lineindex = i\n",
    "                    break\n",
    "                else:\n",
    "                    latindex = line.find(' N ')\n",
    "                    if latindex > -1:\n",
    "                        lineindex = i\n",
    "                        latindex -= 15\n",
    "        if latindex > -1:\n",
    "            latindex += 15\n",
    "            try:\n",
    "                lonindex = list(find_all(lines[lineindex][latindex:latindex+20], 'W'))[0]+latindex\n",
    "                latdeg = float(lines[lineindex][latindex+3:latindex+5])\n",
    "                latmin = float(lines[lineindex][latindex+5:latindex+11])\n",
    "                latdeg = latdeg + latmin/60\n",
    "                londeg = float(lines[lineindex][lonindex+1:lonindex+4])\n",
    "                lonmin = float(lines[lineindex][lonindex+4:lonindex+10])\n",
    "                londeg = -(londeg + lonmin/60)    \n",
    "            except:\n",
    "                print(lineindex, latindex, lines[lineindex]) \n",
    "        #print(latdeg, londeg)\n",
    "        \n",
    "        all_indexes = list(find_all(output, 'POSITION'))\n",
    "\n",
    "        #print(all_indexes)\n",
    "\n",
    "        #print('')\n",
    "\n",
    "        \n",
    "    \n",
    "        for each_index in all_indexes:\n",
    "            \n",
    "            Ndeg = output[each_index+11:each_index+13]\n",
    "            Nmin = output[each_index+14:each_index+16]\n",
    "            Nsec = output[each_index+17:each_index+22]\n",
    "            Ndecdeg = float(Ndeg) + float(Nmin)/60 + float(Nsec)/3600\n",
    "            if Ndecdeg > 28.0 and Ndecdeg < 29.0:\n",
    "                all_lats.append(Ndecdeg)\n",
    "\n",
    "    #print(f\"latdeg={latdeg}, londeg={londeg}, all_lats={all_lats}\")\n",
    "    return latdeg, londeg, all_lats  \n",
    "\n",
    "def create_summary1file(SVCDIR, RAWDIR):\n",
    "    \"\"\" Parse RT130 1/* files to reconstruct digitizer-GPS_Position history\n",
    "    This generates a DataFrame/CSV file like:\n",
    "    \n",
    "    ,Digitizer,yyyyjjj,Latitude,LatSTD\n",
    "    0,92B7,2018222,28.0595,2.8867513458681453e-05\n",
    "    1,92B7,2018223,28.0595,1.8559214542252517e-05\n",
    "    2,92B7,2018229,28.573463888888888,2.721655270429592e-06\n",
    "    3,92B7,2018230,28.573469444444445,1.7899429988238652e-06\n",
    "    4,92B7,2018231,28.573469444444445,2.6032870393506316e-06\n",
    "    5,92B7,2018233,28.573469444444445,3.552713678800501e-15\n",
    "    6,92B7,2018234,28.573469444444445,3.552713678800501e-15\n",
    "    7,92B7,2018235,28.573469444444445,3.552713678800501e-15\n",
    "    \"\"\"\n",
    "        \n",
    "    positionDF = pd.DataFrame(columns = ['Digitizer', 'yyyyjjj', 'Latitude', 'LatSTD'])\n",
    "    digitizerList = list()\n",
    "    yyyyjjjList = list()\n",
    "    latitudeList = list()\n",
    "    latSTDList = list()\n",
    "    longitudeList = list()\n",
    "    lonSTDList = list()\n",
    "    dayfullpaths = sorted(glob.glob('%s/20?????' % RAWDIR))\n",
    "\n",
    "    summary1file = os.path.join(SVCDIR, 'summary1file.csv')\n",
    "    if os.path.exists(summary1file):\n",
    "        os.remove(summary1file)\n",
    "    count = 0\n",
    "\n",
    "    for thisdayfullpath in dayfullpaths:\n",
    "        count = count + 1\n",
    "        #print('Processing %s (%d of %d)' % (thisdayfullpath, count, len(dayfullpaths) ), end=\"\\r\", flush=True)\n",
    "        thisdaydir = os.path.basename(thisdayfullpath) # a directory like 2018365\n",
    "        digitizerpaths = sorted(glob.glob('%s/????' % thisdayfullpath))\n",
    "        for digitizerpath in digitizerpaths:\n",
    "            thisdigitizer = os.path.basename(digitizerpath)\n",
    "            rt130files = sorted(glob.glob('%s/1/*' % digitizerpath))\n",
    "            if rt130files:\n",
    "                all_positions = list()\n",
    "                latdegs = []\n",
    "                londegs = []\n",
    "                for rt130file in rt130files:\n",
    "                    #if not rt130file == '/home/thompsong/work/PROJECTS/KSCpasscal/SVC02/RAW/2019013/91F8/1/090000000_0036EE80':\n",
    "                    #    continue\n",
    "                    #print('Processing %s' % rt130file, end=\"\\r\", flush=True)\n",
    "                    latdeg, londeg, all_lats = read_rt130_1_file(rt130file)\n",
    "                    if latdeg:\n",
    "                        latdegs.append(latdeg)\n",
    "                    if londeg:\n",
    "                        londegs.append(londeg)\n",
    "                    if all_lats:\n",
    "                        if latdeg:\n",
    "                            all_positions.append(latdeg)\n",
    "                        all_positions.extend(all_lats)\n",
    "\n",
    "                digitizerList.append(thisdigitizer)\n",
    "                yyyyjjjList.append(thisdaydir)\n",
    "\n",
    "                if all_positions:  \n",
    "                    all_positions_np = np.array(all_positions)\n",
    "                    #print(all_positions_np)\n",
    "                    #print(np.median(all_positions_np))\n",
    "                    #print('%s, %s, %f, %f\\n' % (thisdigitizer, thisdaydir, np.median(all_positions_np), np.std(all_positions_np)) )\n",
    "\n",
    "                    latitudeList.append(np.nanmedian(all_positions_np))\n",
    "                    latSTDList.append(np.nanstd(all_positions_np))\n",
    "                elif latdegs:\n",
    "                    latitudeList.append(np.nanmedian(latdegs))\n",
    "                    latSTDList.append(np.nanstd(latdegs))\n",
    "                else:\n",
    "                    latitudeList.append(None)\n",
    "                    latSTDList.append(None)                    \n",
    "                if londegs:\n",
    "                    longitudeList.append(np.nanmedian(londegs))\n",
    "                    lonSTDList.append(np.nanstd(londegs))\n",
    "                else:\n",
    "                    longitudeList.append(None)\n",
    "                    lonSTDList.append(None)                    \n",
    "\n",
    "\n",
    "    positionDF['Digitizer'] = digitizerList\n",
    "    positionDF['yyyyjjj'] = yyyyjjjList\n",
    "    if len(latitudeList)>0:\n",
    "        positionDF['Latitude'] = latitudeList\n",
    "        positionDF['LatSTD'] = latSTDList\n",
    "    if len(longitudeList)>0:\n",
    "        positionDF['Longitude'] = longitudeList\n",
    "        positionDF['LonSTD'] = lonSTDList\n",
    "        positionDF = positionDF.round(decimals=5)\n",
    "\n",
    "    positionDF = positionDF.astype({'Digitizer': 'str', 'yyyyjjj': 'str'})\n",
    "    positionDF.to_csv(summary1file, index=False)\n",
    "\n",
    "    return summary1file, positionDF\n",
    "\n",
    "\n",
    "def create_summary9file(SVCDIR, RAWDIR, positionDF):\n",
    "    # Parse RT130 9/* files to reconstruct digitizer-station history\n",
    "    summary9file = os.path.join(SVCDIR, 'summary9file.csv')\n",
    "    lod = []\n",
    "    \n",
    "    STATIONS = ['BHP', 'TANK', 'FIRE', 'BCHH', 'DVEL', 'RBLAB']\n",
    "    dayfullpaths = sorted(glob.glob('%s/20?????' % RAWDIR))\n",
    "    if os.path.exists(summary9file):\n",
    "        os.remove(summary9file)\n",
    "    count = 0\n",
    "    \n",
    "    for thisdayfullpath in dayfullpaths:\n",
    "        count = count + 1\n",
    "        print('Processing %s (%d of %d)' % (thisdayfullpath, count, len(dayfullpaths) ))#, end=\"\\r\", flush=True)\n",
    "        thisdaydir = os.path.basename(thisdayfullpath) # a directory like 2018365\n",
    "        rt130files = sorted(glob.glob('%s/????/9/*' % thisdayfullpath))\n",
    "        \n",
    "        if rt130files:\n",
    "            \n",
    "            for rt130file in rt130files:\n",
    "                output = os.popen('strings %s' % rt130file).read()\n",
    "                if output: \n",
    "                    for station in STATIONS:\n",
    "                        firstindex = output.find(station)\n",
    "                        if firstindex > -1:\n",
    "                            break\n",
    "                        \n",
    "                    if firstindex != -1:\n",
    "                        pathparts = rt130file.split('/')\n",
    "                        rt130 = pathparts[-3]\n",
    "                        lod.append({'Digitizer':rt130, 'yyyyjjj':thisdaydir, 'Station':output[firstindex:firstindex+4].strip()})\n",
    "\n",
    "    if lod:\n",
    "        stationDF = pd.DataFrame(lod)\n",
    "        stationDF = stationDF.astype({'Digitizer': 'str', 'yyyyjjj': 'str'})\n",
    "        combinedDF = pd.merge(positionDF, stationDF, on=['Digitizer', 'yyyyjjj'], how=\"left\")\n",
    "        combinedDF.to_csv(summary9file, index=False)\n",
    "        return summary9file, combinedDF\n",
    "    else:\n",
    "        return '', pd.DataFrame()\n",
    "\n",
    "#############################\n",
    "####  STEP TWO FUNCTIONS ####\n",
    "#############################\n",
    "def commandExists(command):\n",
    "    # commandExists checks if PASSOFT/DMC commands are installed before we try to use them\n",
    "    output = os.popen('which %s' % command).read()\n",
    "    if output:\n",
    "        return True\n",
    "    else:\n",
    "        print('Command %s not found.' % command)\n",
    "        print('Make sure the PASSOFT tools are installed on this computer, and available on the $PATH')\n",
    "        return False\n",
    "\n",
    "def getpaths(SVCDIR):\n",
    "    paths={}\n",
    "    paths['RAWDIR'] = os.path.join(SVCDIR, 'RAW') \n",
    "    paths['LOGSDIR'] =  os.path.join(SVCDIR, 'LOGS')\n",
    "    paths['CONFIGDIR'] = os.path.join(SVCDIR, 'CONFIG')\n",
    "    paths['MSEEDDIR'] = os.path.join(SVCDIR, 'MSEED')\n",
    "    paths['DAYSDIR'] = os.path.join(SVCDIR, 'DAYS')   \n",
    "    return paths\n",
    "\n",
    "def dirsmake(topdir, dirlist):\n",
    "    print(topdir, dirlist)\n",
    "    if not os.path.isdir(topdir):\n",
    "        try:\n",
    "            print(f'Attempting to make {topdir}')\n",
    "            os.makedirs(topdir)\n",
    "        except:\n",
    "            print(\"%s does not exist. Exiting\" % topdir)\n",
    "            raise SystemExit(\"Killed!\") \n",
    "\n",
    "    for thissubdir in dirlist:\n",
    "        if not os.path.exists(thissubdir):\n",
    "            print('Need to make %s' % thissubdir)\n",
    "            os.mkdir(thissubdir)\n",
    "            if not os.path.isdir(thissubdir):\n",
    "                print(\"%s does not exist & could not be created. Exiting\" % thissubdir)\n",
    "                raise SystemExit(\"Killed!\")         \n",
    "#rt130file = os.path.join(REFTEKDIR, 'SVC06', 'RAW', '2019191', '92B7', '1', '000000000_0036EE80')\n",
    "#read_rt130_1_file(rt130file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a fudge after writing all the code below to use all the data from the PASSCAL laptop backup, which seems the most complete\n",
    "\n",
    "ALTREFTEKDIR = '/data/KSC/duringPASSCAL/PASSCAL_laptop_backup'\n",
    "alldaysdir = os.path.join(ALTREFTEKDIR, 'REFTEK_DATA', 'SORTED')\n",
    "REFTEKDIR = '/home/thompsong/work/PROJECTS/KSCpasscal'\n",
    "svcalldir = os.path.join(REFTEKDIR, 'CONVERT', 'SVCall')\n",
    "paths = getpaths(svcalldir)\n",
    "dirlist=[]\n",
    "for k in paths:\n",
    "    dirlist.append(paths[k])\n",
    "dirsmake(svcalldir, dirlist)\n",
    "os.system(f\"cp -rn {alldaysdir}/* {paths['RAWDIR']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"mv {paths['RAWDIR']}/SORTED/* {paths['RAWDIR']}/\")\n",
    "os.system(f\"rmdir {paths['RAWDIR']}/SORTED/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1. Create summary files and reconstruct digitizer-station-latitude versus time combinations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFTEKDIR = '/home/thompsong/work/PROJECTS/KSCpasscal'\n",
    "\n",
    "#ServiceRunDirs = sorted(glob.glob(os.path.join(REFTEKDIR, 'SVC??')))\n",
    "ServiceRunDirs = sorted(glob.glob(os.path.join(REFTEKDIR, 'CONVERT', 'SVCall'))) # hack for \n",
    "print(ServiceRunDirs)\n",
    "for SVCDIR in ServiceRunDirs:\n",
    "    print('Setting paths for relative directories/files')\n",
    "    paths = getpaths(SVCDIR)\n",
    "\n",
    "    print('Creating outline directory structure')\n",
    "    dirlist=[]\n",
    "    for k in paths:\n",
    "        dirlist.append(paths[k])\n",
    "    dirsmake(REFTEKDIR, dirlist)\n",
    "    print('Outline directory structure created/exists')\n",
    "\n",
    "    # Create summary files from 1/* and 9/* files\n",
    "    summary1file, positionDF = create_summary1file(SVCDIR, paths['RAWDIR'])\n",
    "    if len(positionDF)>0:\n",
    "        summary9file, combinedDF = create_summary9file(SVCDIR, paths['RAWDIR'], positionDF)\n",
    "        if not combinedDF.empty:\n",
    "            display(combinedDF)\n",
    "            # march through the dataframe chronologically, finding any statistically significant changes in GPS coordinates from one day to another for same digitizer-station combo\n",
    "        else:\n",
    "            display(positionDF)\n",
    "            # march through the dataframe chronologically, finding any changes from one day to another in digitizer-station combo\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2. Create the parameter file(s) and run rt2ms\n",
    "The parameter file is used by *rt2ms* to assign header information to the miniSEED files. *rt2ms* is a PASSCAL program that generates miniSEED formatted files from REFTEK RT130 raw files. In addition, *rt2ms* also modifies the headers. In the SVC1 directory, use a text editor and information from your field notes to create an ASCII parameter file (parfile) following the examples at https://www.passcal.nmt.edu/webfm_send/3035.\n",
    "\n",
    "<b>Glenn's variations</b>: PASSCAL instructions assume you construct a par file by hand for each network layout. However, I construct an Antelope-style *dbbuild_batch* pf file by hand for each network layout, and then use the PASSOFT program *batch2par* to convert this to a par file (in combination with 2 *sed* (Unix stream editor) commands to fix this. You can see *batch2par* and *sed* commands used below.\n",
    "\n",
    "<b>Update 20241110</b>\n",
    "Today I downloaded and installed the latest conda version of passoft3, and this no longer includes batch2par. Thus, I need to hand-edit the par files.\n",
    "Furthermore, rt2ms has changed. It no longer has -F, -L or -Y command line arguments. I'm trying to work out how to use it - it seems to require a different folder organization, as it doesn't like multiple digitizer folders in the same YYYYJJJ directory.\n",
    "And finally, the *.par format has changed. A blank 'location' columns is now needed, as in network.station.location.channel (SEED id). And a final 'implment_time' column is needed in UTCDateTime.isoformat() with 6-digit microseconds and perhaps a 'Z' on end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy\n",
    "ServiceRunDirs = sorted(glob.glob(os.path.join(REFTEKDIR, 'SVC??')))\n",
    "for SVCDIR in ServiceRunDirs:\n",
    "    print('Setting paths for relative directories/files')\n",
    "    paths = getpaths(SVCDIR)\n",
    "    print(paths)\n",
    "    #RT2MS_OUTPUT = os.path.join(LOGSDIR, 'rt2ms.out')\n",
    "    \n",
    "    # For each dbbuild_batch pf file, ensure there is a corresponding par file (create it if batch2par program is installed\n",
    "    pffile = sorted(glob.glob('%s/locations*.pf' % paths['CONFIGDIR']))[0] # changed network to locations. assume there is only one file per SVCDIR\n",
    "    parfile = pffile[:-2] + 'par' # \n",
    "    print('- %s, %s' % (pffile, parfile)) \n",
    "            \n",
    "    # Create the corresponding parfile if it does not already exist\n",
    "    if not os.path.exists(parfile): \n",
    "        if commandExists('batch2par'): \n",
    "            os.system(\"batch2par %s -m > %s\" % (pffile, parfile))\n",
    "            if os.path.exists(parfile): \n",
    "                # Edit the par file\n",
    "                os.system(\"sed -i -e 's/rs200spsrs;/1;         /g' %s\" % parfile)\n",
    "                os.system(\"sed -i -e 's/x1/32/g' %s\" % parfile);  \n",
    "            else:\n",
    "                print(\"- batch2par failed\")\n",
    "                raise SystemExit(\"Killed!\")\n",
    "        else:\n",
    "            raise SystemExit(\"batch2par program is not installed\")\n",
    "\n",
    "\n",
    "    # Convert par file to new rt2ms compatible format with location and implement_time columns\n",
    "    #parfile = sorted(glob.glob('%s/locations*.par' % CONFIGDIR))[0] # changed network to locations\n",
    "    os.system(f\"cat {parfile}\")\n",
    "    tmpparfile = parfile.replace('.par', '.par.reformatted')\n",
    "    os.system(f'sed -e \"s/[[:space:]]\\+/ /g\"  {parfile} > {tmpparfile}')\n",
    "    print('removing comment\\n')\n",
    "    tmpparfile2 = parfile.replace('.par', '.par.uncommented')\n",
    "    os.system(f'sed -e \"1s/#//\" {tmpparfile} > {tmpparfile2}')\n",
    "    df = pd.read_csv(tmpparfile2, sep=';', index_col=None)\n",
    "    df['location']=''\n",
    "    datestr = os.path.basename(parfile).replace('locations','').replace('.par','')\n",
    "    df['implement_time'] = obspy.UTCDateTime.strptime(datestr, '%Y%m%d').isoformat()+'.000000Z'\n",
    "    newparfile = parfile.replace('.par', '.par.new')\n",
    "    df = df[['das',' refchan', ' refstrm', ' netcode', ' station', 'location', ' channel', ' samplerate', ' gain', 'implement_time']]\n",
    "    df.to_csv(newparfile, index=False, sep=';')\n",
    "    os.system(f\"sed -i 's/\\;/\\; /g; s/\\;\\s\\+/\\; /g' {newparfile}\")\n",
    "    os.system(f\"cat {newparfile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Convert your data into miniSEED files. \n",
    "In the service run directory, convert the raw RT130 data to miniSEED. Typing *rt2ms -h* shows a list of available options. \n",
    "\n",
    "If raw data is in decompressed folders, use the following commands: \n",
    "\n",
    "    ls -d SVC1/RAW/*.cf > file.lst rt2ms -F file.lst -Y -L -o MSEED/ -p <parfile> >& rt2ms.out \n",
    "\n",
    "The (-F) flag will process all files in the named list, (‐Y) puts the data in yearly directories, (-L) outputs .log and, if created, .err files, (‐o) creates an output directory, MSEED, and (‐p) points to your parfile. \n",
    "\n",
    "If raw data is in ZIP files: \n",
    "\n",
    "    rt2ms ‐D SVC1/RAW/ ‐Y ‐L -o MSEED/ ‐p <parfile> >& rt2ms.out \n",
    "\n",
    "The (‐D) flag will process all .ZIP files in a specified directory, instead of in a file list as in the previous example. \n",
    "\n",
    "When *rt2ms* finishes, move all of your log and .err files from the MSEED directory to the LOGS directory that you created in step 1. \n",
    "\n",
    "After running *rt2ms* the MSEED directory structure should look something like the example below. In the MSEED directory there will be .log files and possibly .err files along with a  subdirectory for each year that contains day directories for each stream.\n",
    "\n",
    "<pre>\n",
    "    MSEED/\n",
    "    2014.019.21.29.16.98EZ.log\n",
    "    2014.019.21.29.16.98EZ.err\n",
    "    Y2014/\n",
    "        R065.01/\n",
    "        R065.02/\n",
    "        R065.03/\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ServiceRunDirs = sorted(glob.glob(os.path.join(REFTEKDIR, 'SVC??')))\n",
    "for SVCDIR in ServiceRunDirs:\n",
    "    print('Setting paths for relative directories/files')\n",
    "    paths = getpaths(SVCDIR)\n",
    "    print(paths)\n",
    "    #RT2MS_OUTPUT = os.path.join(LOGSDIR, 'rt2ms.out')\n",
    "\n",
    "    NEWSVCDIR = os.path.join(REFTEKDIR, 'CONVERT', os.path.basename(SVCDIR))\n",
    "    newpaths = getpaths(NEWSVCDIR)\n",
    "    print('Creating outline directory structure')\n",
    "    dirlist=[]\n",
    "    for k in newpaths:\n",
    "        dirlist.append(newpaths[k])\n",
    "    dirsmake(NEWSVCDIR, dirlist)\n",
    "    \n",
    "    parfile = sorted(glob.glob(os.path.join(paths['CONFIGDIR'], '*.par.new')))[0]\n",
    "    newparfile = os.path.join(newpaths['CONFIGDIR'], os.path.basename(parfile))\n",
    "    os.system(f\"cp {parfile} {newparfile}\")\n",
    "                           \n",
    "    \"\"\" Build a temporary folder structure so that we can process it with new rt2ms which expects data straight of a CompactFlash card\n",
    "    e.g. TEST/YYYYJJJ/datalogger, but for just one datalogger\n",
    "    \"\"\"\n",
    "    summary1file = os.path.join(SVCDIR, 'summary1file.csv')\n",
    "    if os.path.isfile(summary1file):\n",
    "        positionDF = pd.read_csv(summary1file, index_col=None)\n",
    "        allDigitizers = list(set(positionDF['Digitizer'].to_list()))\n",
    "    else:\n",
    "        allDigitizers = []\n",
    "    for datalogger in allDigitizers:\n",
    "        dayfullpaths = sorted(glob.glob('%s/20?????' % paths['RAWDIR']))\n",
    "        CFDIR = os.path.join(newpaths['RAWDIR'], f'RT130-{datalogger}-1.cf')\n",
    "        if not os.path.isdir(CFDIR):\n",
    "            os.makedirs(CFDIR)\n",
    "        \n",
    "        for thisdayfullpath in dayfullpaths:\n",
    "            print('Processing %s' % thisdayfullpath)\n",
    "            thisdaydir = os.path.basename(thisdayfullpath) # a directory like 2018365\n",
    "            dataloggerdir = os.path.join(thisdayfullpath, datalogger)\n",
    "            outputdir = os.path.join(CFDIR, thisdaydir)\n",
    "            print(dataloggerdir, '\\t->\\t',outputdir)\n",
    "            if os.path.isdir(dataloggerdir):\n",
    "                if not os.path.isdir(outputdir):\n",
    "                    os.makedirs(outputdir)\n",
    "                \n",
    "                os.system(f\"cp -rn {dataloggerdir} {outputdir}\")\n",
    "\n",
    "        \"\"\"\n",
    "        # run rt2ms\n",
    "        RT2MS_OUTPUT = os.path.join(CFDIR, 'rt2ms.out')\n",
    "        cmd = f\"rt2ms -d {CFDIR} -p {newparfile} -o {newpaths['MSEEDDIR']} > {RT2MS_OUTPUT}\"\n",
    "        print(cmd)\n",
    "        os.system(cmd)    \n",
    "\n",
    "        # move all *.log files to the LOGS directory\n",
    "        for src_file in Path(newpaths['MSEEDDIR']).glob('*.log'):\n",
    "            shutil.copy(src_file, newpaths['LOGSDIR'])\n",
    "\n",
    "        # move all *.err files to the LOGS directory\n",
    "        for src_file in Path(newpaths['MSEEDDIR']).glob('*.err'):\n",
    "            shutil.copy(src_file, newpaths['LOGSDIR'])  \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When *rt2ms* finishes, move all of your log and .err files from the MSEED directory to the LOGS directory that you created in step 1. \n",
    "\n",
    "After running *rt2ms* the MSEED directory structure should look something like the example below. In the MSEED directory there will be .log files and possibly .err files along with a  subdirectory for each year that contains day directories for each stream."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MSEED/\n",
    "    2014.019.21.29.16.98EZ.log\n",
    "    2014.019.21.29.16.98EZ.err\n",
    "    Y2014/\n",
    "        R065.01/\n",
    "        R065.02/\n",
    "        R065.03/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We also want to track the digitizer/lat/lon/station changes from the summary9file</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ServiceRunDirs = sorted(glob.glob(os.path.join(REFTEKDIR, 'SVC??')))\n",
    "summaryallcsv = os.path.join(REFTEKDIR, 'CONVERT', 'summaryall.csv')\n",
    "lod = []\n",
    "for SVCDIR in ServiceRunDirs:\n",
    "    print('Setting paths for relative directories/files')\n",
    "    paths = getpaths(SVCDIR)\n",
    "    NEWSVCDIR = os.path.join(REFTEKDIR, 'CONVERT', os.path.basename(SVCDIR))\n",
    "    newpaths = getpaths(NEWSVCDIR)\n",
    "    \n",
    "    # Create summary files from 1/* and 9/* files\n",
    "    summary1file = os.path.join(SVCDIR, 'summary1file.csv')\n",
    "    summary9file = os.path.join(SVCDIR, 'summary9file.csv')\n",
    "    \n",
    "    if os.path.isfile(summary9file):\n",
    "        shutil.copy(summary9file, os.path.join(newpaths['CONFIGDIR'], 'summary9file.csv'))\n",
    "        df = pd.read_csv(summary9file, index_col=None)\n",
    "    else:\n",
    "        shutil.copy(summary1file, os.path.join(newpaths['CONFIGDIR'], 'summary1file.csv'))\n",
    "        df = pd.read_csv(summary1file, index_col=None)\n",
    "\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    #print(df)\n",
    "    if 'Station' in df.columns:\n",
    "        df['Station'] = df['Station'].astype(str)\n",
    "    for digitizer in df['Digitizer'].unique():\n",
    "        subdf = df[df['Digitizer']==digitizer]  \n",
    "        if not 'Station' in subdf.columns:\n",
    "            print(NEWSVCDIR, digitizer, 'no stations')\n",
    "        else:\n",
    "            for i,row in subdf.iterrows(): # not doing anything\n",
    "                if not isinstance(row['Station'], str) and row['Station'].isnull():\n",
    "                    subdf.at[i, 'Station'] = 'Unknown'\n",
    "                     \n",
    "            for station in subdf['Station'].unique():\n",
    "                stationdf = subdf[subdf['Station']==station]\n",
    "            \n",
    "                print(NEWSVCDIR, digitizer, station, stationdf.iloc[0]['yyyyjjj'], stationdf.iloc[-1]['yyyyjjj'], \n",
    "                      f\"{stationdf['Latitude'].median():.5f}, {stationdf['Longitude'].median():.5f}, \\\n",
    "                      {stationdf['Latitude'].std():.5f}, {stationdf['Longitude'].std():.5f}\")\n",
    "                if not isinstance(station, str):\n",
    "                    station = 'Unknown'\n",
    "                thisdict = {'Digitizer':digitizer, 'Station':station, 'StartDate':stationdf.iloc[0]['yyyyjjj'], \n",
    "                            'EndDate':stationdf.iloc[-1]['yyyyjjj'], \n",
    "                            'MedianLatitude':f\"{stationdf['Latitude'].median():.5f}\", \n",
    "                            'MedianLongitude': f\"{stationdf['Longitude'].median():.5f}\"}            \n",
    "                lod.append(thisdict)\n",
    "            outdf = pd.DataFrame(lod)\n",
    "            outdf.to_csv(summaryallcsv, index=False)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Print the first and last dates of each digitizer-station combo\n",
    "    output = os.popen('sort %s | uniq' % summary9file).read()\n",
    "    #print(output)\n",
    "    lastStationDigitizerCombo = \"\"\n",
    "    lastline = \"\"\n",
    "    for thisline in output.split('\\n'):\n",
    "        try:\n",
    "            (station, yyyyjjj, digitizer)  = thisline.split(',')\n",
    "            thisStationDigitizerCombo = '%s%s' % (station, digitizer)\n",
    "            #print(lastStationDigitizerCombo, thisStationDigitizerCombo)\n",
    "            if lastStationDigitizerCombo != thisStationDigitizerCombo:\n",
    "                print(lastline)\n",
    "                print(thisline)\n",
    "            lastStationDigitizerCombo = thisStationDigitizerCombo\n",
    "            lastline = thisline\n",
    "        except:\n",
    "            pass\n",
    "    print(lastline)\n",
    "\n",
    "    ####################################################################################################\n",
    "    # Detect dates on which a digitizer was moved by examining plots of GPS Latitude & standard deviation.\n",
    "    ####################################################################################################\n",
    "\n",
    "    # Convert yyyyjjj strings to datetime objects\n",
    "    dates = list()\n",
    "    for thisyyyyjjj in positionDF['yyyyjjj']:\n",
    "        thisdate = datetime.datetime.strptime(thisyyyyjjj, \"%Y%j\").date()\n",
    "        dates.append(thisdate)\n",
    "    positionDF['dates']=dates\n",
    "\n",
    "    allDigitizers = list(set(positionDF['Digitizer'].to_list()))\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    for digitizer in allDigitizers:\n",
    "        subsetDF = positionDF[positionDF['Digitizer']==digitizer]\n",
    "        ax.plot_date(subsetDF['dates'], subsetDF['Latitude'],'.', label=digitizer)  \n",
    "        \n",
    "    #xt = ax.get_xticks()\n",
    "    #ax.set_xticks()\n",
    "    ax.set_xlim(dates[0],dates[-1])\n",
    "    for xtl in ax.get_xticklabels():\n",
    "        xtl.set_rotation(30)\n",
    "        xtl.set_horizontalalignment('right')\n",
    "    plt.legend()\n",
    "    plt.title(os.path.basename(SVCDIR))\n",
    "    ax.set_ylim(28.51, 28.58)\n",
    "    plt.savefig('digitizer_lats.png')\n",
    "    ax.set_ylim(28.572, 28.575)\n",
    "    plt.savefig('digitizer_lats_zoomed.png')\n",
    "    #plt.tight_layout()    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "(I have not reworked the following code yet)\n",
    "<hr/>\n",
    "\n",
    "## STEP 4: Reorganize the miniSEED data into station/channel/day volumes.\n",
    "\n",
    "*dataselect* is a DMC program that allows for the extracting and sorting of miniSEED data (https://github.com/iris-edu/dataselect). This will read the data from the MSEED directory and convert them into day volumes with the required naming format: \n",
    "\n",
    "    dataselect -A DAYS/%s/%s.%n.%l.%c.%Y.%j MSEED/Y*/*/* \n",
    "\n",
    "The (-A) flag writes file names in the specified custom format. The format flags are (s) for station, (n) for netcode, (l) for location, (c) for channel name, (Y) for year, and (j) for Julian date. See the help menu for more details on options (*dataselect -h*). Depending on how much data you have, you may need to run *dataselect* in a loop that runs over the different days or stations in your experiment.\n",
    "\n",
    "<b>Please note:</b> PASSCAL want data to be organized in BUD format. *dataselect -h* reveals that there is a BUD format option built in directly, so I attempt to use that here instead. This command is easier: \n",
    "\n",
    "    dataselect -BUD MSEED/Y*/*/* \n",
    "    \n",
    "However, I do loop over year and day directories (as suggested) so that *dataselect* is not trying to deal with a file list that is too long for it or the operating system to handle.\n",
    "\n",
    "(Or I could do this in ObsPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if commandExists('dataselect'): \n",
    "    # SCAFFOLD. I need to install this from github. And then figure out how to\n",
    "    # substitute for station, netcode, location, channel name, year and julian day\n",
    "    # May need to lop over different stations (I already do by day)\n",
    "    mseedyearfullpaths = sorted(glob.glob('%s/Y20??' % MSEEDDIR))\n",
    "    for thismseedyearfullpath in mseedyearfullpaths:\n",
    "        print('Processing %s' % thismseedyearfullpath)  \n",
    "        mseeddayfullpaths = sorted(glob.glob('%s/R*.01' % thismseedyearfullpath))\n",
    "        for thismseeddayfullpath in mseeddayfullpaths:\n",
    "            print('dataselect: Processing %s' % thismseeddayfullpath)\n",
    "            #os.system('dataselect  -A %s/%s/%s.%n.%l.%c.%Y.%j  %s/*.m' % (DAYSDIR, thismseeddayfullpath))\n",
    "            os.system('dataselect -v -BUD %s %s/*.m' % (DAYSDIR, thismseeddayfullpath) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: Confirm your station and channel names\n",
    "In the DAYS folder just created by dataselect, check to see if you have folders for each of your stations. The data should be organized into those folders in station/channel/day volumes named STA.NET.LOC.CHAN.YEAR.JULDAY. For example: BA01.XR..HHZ.2018.039 (The .. after XR is where the location code would be if needed).\n",
    "\n",
    "If your parfile was incomplete (i.e. missing stations or channels), there will be one or more folders named with the RT130 serial numbers (e.g. 9306) instead of the desired station name (e.g. ME42). To change any miniSEED headers to correct a station name, network code, etc., see the *fixhdr* doc on the PASSCAL website (see link on the first page). After you have modified the headers with *fixhdr*, rename the files so that the station‐network‐location‐channel codes in the miniSEED file names match the corrected headers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6. Perform quality control of waveforms and logs. \n",
    "Verify the data quality by reviewing the traces and log files (with *logpeek* and *pql*). Obvious signs of trouble include loss of GPS timing, overlaps, gaps, corrupted files, etc. Make a note of any problems. Use *fixhdr* to correct mark timing issues, and/or to convert the files to big endianess if they are not already. For more information on how to use these tools, refer to the appropriate documentation on the PASSCAL website (see link on the first page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7. Create metadata for your experiment. \n",
    "Use *Nexus* to generate a stationXML file for your experiment metadata. See the “Metadata Generation with Nexus in a Nutshell” document on the PASSCAL website (see link on first page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8. Send miniSEED data to PASSCAL. \n",
    "Please drop a note, with your PASSCAL project name in the subject, to <mailto>data_group@passcal.nmt.edu</mailto> before sending the data to PASSCAL so that we can set up a receiving area. Attach the stationXML created with Nexus to this email unless it is larger than 5Mb. Use our tool *data2passcal* to send the data: \n",
    "\n",
    "    data2passcal DAYS/ \n",
    "\n",
    "*data2passcal* will scan all subdirectories of the DAYS folder and send any miniSEED files that have the correct file names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
