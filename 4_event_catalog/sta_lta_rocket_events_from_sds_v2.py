#!/usr/bin/env python3
"""
sta_lta_rocket_events_from_sds.py

Loop through rocket launch time windows from a CSV, pad the windows,
pull data from an SDS archive using flovopy.sds.sds.SDSobj (or fallback to ObsPy SDSClient),
and run ObsPy STA/LTA to detect (a) the main launch signal and (b) a potential
booster landing signal returning to KSC.

Outputs a results CSV with detected trigger times and metrics per event/station.
Optionally saves quicklook PNGs.

CSV requirements (flexible):
- Either provide `start_time` and `end_time` columns (ISO-8601 or "YYYY-MM-DD HH:MM:SS")
- Or provide `t0` (launch epoch or ISO) plus `window_seconds` (duration)
Optional identifier columns (copied through): event_id, mission, rocket, pad, notes, etc.

Example:
    python sta_lta_rocket_events_from_sds.py \
        --csv /data/events.csv \
        --sds-root /data/remastered/SDS_KSC \
        --stations "1R.TANK.00.DHZ,1R.CARL1.00.DHZ" \
        --pad-before 60 --pad-after 600 \
        --sta 1.0 --lta 20.0 --on 3.5 --off 1.5 \
        --max-two-triggers \
        --out /data/results_sta_lta.csv \
        --quicklooks /data/quicklooks

Author: (generated by ChatGPT for Glenn)
"""

import os
import sys
import argparse
import warnings
from typing import List, Optional, Tuple

import numpy as np
import pandas as pd
from obspy import UTCDateTime, read
from obspy.signal.trigger import classic_sta_lta, trigger_onset

# Try flovopy SDSobj; if not available, we fallback to ObsPy SDS Client
SDSOBJ_OK = True
try:
    from flovopy.sds.sds import SDSobj
except Exception as e:
    SDSOBJ_OK = False
    SDSOBJ_IMPORT_ERR = e

FALLBACK_OK = True
try:
    from obspy.clients.filesystem.sds import Client as SDSClient
except Exception as e:
    FALLBACK_OK = False
    FALLBACK_IMPORT_ERR = e

import matplotlib
matplotlib.use("Agg")  # for headless environments
import matplotlib.pyplot as plt


def parse_station_list(stations_str: str) -> List[Tuple[str, str, str, str]]:
    """
    Parse a comma-separated list of trace IDs into (net, sta, loc, cha) tuples.
    Accepts forms like:
        1R.TANK.00.DHZ
        1R.TANK..DHZ      (empty loc)
        AM.R37BE.00.HDF
    """
    items = []
    for part in stations_str.split(","):
        part = part.strip()
        if not part:
            continue
        fields = part.split(".")
        if len(fields) != 4:
            raise ValueError(f"Station spec must be NET.STA.LOC.CHA: got '{part}'")
        net, sta, loc, cha = fields
        loc = "" if loc in ("--", ".", "*", "  ", None) else loc
        items.append((net, sta, loc, cha))
    return items


def to_utc(val) -> UTCDateTime:
    """Robustly parse a time field that may be ISO string or epoch float/int."""
    if pd.isna(val):
        raise ValueError("Missing time value")
    # Try float epoch
    try:
        return UTCDateTime(float(val))
    except Exception:
        # Try string
        return UTCDateTime(str(val))


def load_events(csv_path: str) -> pd.DataFrame:
    df = pd.read_csv(csv_path)
    # Normalize possible time columns. Support window_start/window_end used in Launch Library exports.
    if {"start_time", "end_time"} <= set(df.columns):
        df["t_start"] = df["start_time"].apply(to_utc)
        df["t_end"] = df["end_time"].apply(to_utc)
    elif {"window_start", "window_end"} <= set(df.columns):
        df["t_start"] = df["window_start"].apply(to_utc)
        df["t_end"] = df["window_end"].apply(to_utc)
    elif {"t0", "window_seconds"} <= set(df.columns):
        df["t0"] = df["t0"].apply(to_utc)
        df["t_start"] = df["t0"]
        df["t_end"] = df["t0"] + df["window_seconds"].astype(float)
    else:
        raise ValueError(
            "CSV must contain either start_time & end_time, window_start & window_end, or t0 & window_seconds"
        )
    return df


def get_stream_from_sds(sds_root: str, net: str, sta: str, loc: str, cha: str,
                        t1: UTCDateTime, t2: UTCDateTime, speed: int = 3):
    """
    Get an ObsPy Stream from SDS using flovopy.sds.sds.SDSobj if available,
    otherwise use ObsPy's SDSClient as a fallback.
    """
    # Prefer flovopy
    if SDSOBJ_OK:
        try:
            sds = SDSobj(sds_root)
            # Many wrappers emulate ObsPy SDSClient
            st = sds.client.get_waveforms(net, sta, loc or "", cha, t1, t2, cleanup=True)
            return st
        except Exception as e:
            warnings.warn(f"SDSobj failed ({e}); trying ObsPy SDSClient fallback...")

    if FALLBACK_OK:
        client = SDSClient(sds_root)
        st = client.get_waveforms(net, sta, loc or "", cha, t1, t2)
        return st

    # If neither import worked:
    if SDSOBJ_OK is False and FALLBACK_OK is False:
        raise ImportError(f"Cannot access SDS: flovopy error={SDSOBJ_IMPORT_ERR}, "
                          f"ObsPy fallback error={FALLBACK_IMPORT_ERR}")
    raise RuntimeError("Unable to acquire waveforms from SDS")


def run_sta_lta_on_trace(tr, sta_s: float, lta_s: float, on: float, off: float,
                         max_trigs: int = 2, t0_hint: Optional[UTCDateTime] = None,
                         min_sep_s: float = 30.0):
    """
    Run classic STA/LTA on a single Trace and return up to `max_trigs` trigger onsets.
    Returns a list of dicts with onset/offset indices & times and max STA/LTA value near onset.
    """
    sr = tr.stats.sampling_rate
    if sr is None or sr <= 0:
        return []

    nsta = max(1, int(round(sta_s * sr)))
    nlta = max(nsta + 1, int(round(lta_s * sr)))
    cft = classic_sta_lta(tr.data.astype(float), nsta, nlta)

    # Get all on/off pairs
    on_off = trigger_onset(cft, on, off)
    if on_off is None or len(on_off) == 0:
        return []

    # Rank triggers by peak CFT near onset
    candidates = []
    for (i_on, i_off) in on_off:
        i0 = max(0, i_on - int(1 * sr))
        i1 = min(len(cft) - 1, i_on + int(5 * sr))
        peak_val = float(np.nanmax(cft[i0:i1])) if i1 > i0 else float(np.nanmax(cft))
        t_on = tr.stats.starttime + (i_on / sr)
        t_off = tr.stats.starttime + (i_off / sr)
        dt_hint = abs((t_on - t0_hint)) if t0_hint else None
        candidates.append({
            "i_on": int(i_on), "i_off": int(i_off),
            "t_on": t_on, "t_off": t_off,
            "peak_cft": peak_val,
            "dt_from_hint": float(dt_hint) if dt_hint is not None else None
        })

    # Sort: prefer closeness to hint first (if provided), then peak_cft descending
    if t0_hint is not None:
        candidates.sort(key=lambda d: (d["dt_from_hint"], -d["peak_cft"]))
    else:
        candidates.sort(key=lambda d: -d["peak_cft"])

    # Pick up to max_trigs, enforcing min separation
    picks = []
    for cand in candidates:
        if not picks:
            picks.append(cand)
            continue
        if all(abs((cand["t_on"] - p["t_on"])) >= min_sep_s for p in picks):
            picks.append(cand)
        if len(picks) >= max_trigs:
            break

    return picks


def save_quicklook(tr, picks, out_png):
    """
    Save a simple waveform + STA/LTA trigger overlay quicklook.
    """
    try:
        sr = tr.stats.sampling_rate
        t = np.arange(tr.stats.npts) / sr
        fig = plt.figure(figsize=(10, 4))
        ax = plt.gca()
        ax.plot(t, tr.data, linewidth=0.8)
        for k, p in enumerate(picks, 1):
            ton = (p["t_on"] - tr.stats.starttime)
            ax.axvline(ton, linestyle="--", linewidth=1.0, label=f"Trig {k}")
        ax.set_xlabel("Time (s) since window start")
        ax.set_ylabel("Amplitude")
        ax.legend(loc="upper right")
        ax.set_title(f"{tr.id}  {tr.stats.starttime} to {tr.stats.endtime}")
        fig.tight_layout()
        fig.savefig(out_png, dpi=150)
        plt.close(fig)
    except Exception as e:
        warnings.warn(f"Quicklook save failed for {tr.id}: {e}")



def discover_all_trace_ids_with_sdsobj(sds_root: str, intervals, skip_low_rate_channels: bool=True, speed: int=3):
    """
    Discover non-empty trace IDs in the SDS archive over a set of (t1, t2) UTCDateTime intervals
    using flovopy.sds.sds.SDSobj._get_nonempty_traceids. Returns a sorted list of strings
    like 'NET.STA.LOC.CHA'.
    """
    if not SDSOBJ_OK:
        raise ImportError("flovopy.sds.sds.SDSobj is required for automatic station discovery.")
    sds = SDSobj(sds_root)
    found = set()
    for (t1, t2) in intervals:
        start_epoch = float(t1.timestamp)
        end_epoch = float(t2.timestamp)
        try:
            tids = sds._get_nonempty_traceids(start_epoch, end_epoch,
                                              skip_low_rate_channels=skip_low_rate_channels,
                                              speed=speed)
            for tid in tids:
                found.add(tid)
        except Exception as e:
            import warnings
            warnings.warn(f"Trace discovery failed for interval {t1}â€“{t2}: {e}")
            continue
    return sorted(found)


def parse_trace_id_string(tid: str):
    """Split 'NET.STA.LOC.CHA' into (net, sta, loc, cha), normalizing empty loc."""
    parts = tid.split('.')
    if len(parts) != 4:
        raise ValueError(f"Bad trace id '{tid}' (expected NET.STA.LOC.CHA)")
    net, sta, loc, cha = parts
    loc = '' if loc in ('--', '.', '*', '  ', None) else loc
    return net, sta, loc, cha


def main():
    ap = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    ap.add_argument("--csv", required=True, help="CSV of event windows")
    ap.add_argument("--sds-root", required=True, help="Root of SDS archive")
    ap.add_argument("--stations", required=False,
                    help="Comma list of NET.STA.LOC.CHA (e.g., '1R.TANK.00.DHZ,1R.CARL1.00.DHZ')")
    ap.add_argument("--pad-before", type=float, default=60.0, help="Seconds before window start")
    ap.add_argument("--pad-after", type=float, default=600.0, help="Seconds after window end")
    ap.add_argument("--sta", type=float, default=1.0, help="STA window (s)")
    ap.add_argument("--lta", type=float, default=20.0, help="LTA window (s)")
    ap.add_argument("--on", type=float, default=3.5, help="Trigger on threshold")
    ap.add_argument("--off", type=float, default=1.5, help="Trigger off threshold")
    ap.add_argument("--max-two-triggers", action="store_true",
                    help="Return up to two triggers (e.g., launch + booster)")
    ap.add_argument("--min-sep", type=float, default=45.0,
                    help="Minimum separation (s) between triggers when selecting two")
    ap.add_argument("--quicklooks", default=None,
                    help="Directory to save quicklook PNGs (optional)")
    ap.add_argument("--out", required=True, help="Output results CSV path")
    args = ap.parse_args()

    # Load events
    df = load_events(args.csv)

    
    # Determine stations
    if args.stations:
        stations = parse_station_list(args.stations)
    else:
        # Build padded intervals from events
        intervals = []
        for _idx, _ev in df.iterrows():
            _t1 = _ev["t_start"] - args.pad_before
            _t2 = _ev["t_end"] + args.pad_after
            intervals.append((_t1, _t2))
        # Discover all trace ids with flovopy SDSobj
        tid_list = discover_all_trace_ids_with_sdsobj(args.sds_root, intervals, skip_low_rate_channels=True, speed=3)
        if not tid_list:
            raise RuntimeError("No non-empty trace IDs discovered in SDS for the provided windows.")
        stations = [parse_trace_id_string(tid) for tid in tid_list]


    # Prepare output
    rows = []
    os.makedirs(os.path.dirname(args.out), exist_ok=True)
    ql_dir = None
    if args.quicklooks:
        ql_dir = args.quicklooks
        os.makedirs(ql_dir, exist_ok=True)

    for idx, ev in df.iterrows():
        t1 = ev["t_start"] - args.pad_before
        t2 = ev["t_end"] + args.pad_after

        # Event IDs/labels to carry through (avoid collisions with trace keys like net/sta/loc/cha)
        reserved = {"t_start", "t_end", "net", "sta", "loc", "cha", "window_start", "window_end", "start_time", "end_time"}
        ev_meta = {}
        for col in df.columns:
            if col in {"t_start", "t_end"}:
                continue
            key = f"event_{col}" if col in reserved else col
            ev_meta[key] = ev[col]


        for (net, sta, loc, cha) in stations:
            try:
                st = get_stream_from_sds(args.sds_root, net, sta, loc, cha, t1, t2)
                if len(st) == 0:
                    raise RuntimeError("Empty stream")
                st.merge(method=1, fill_value="interpolate")
                st.detrend("constant")
                st.taper(0.01)

                tr = st[0]  # single-channel
                picks = run_sta_lta_on_trace(
                    tr, args.sta, args.lta, args.on, args.off,
                    max_trigs=2 if args.max_two_triggers else 1,
                    t0_hint=ev.get("t0", ev["t_start"]),
                    min_sep_s=args.min_sep,
                )

                # Optionally save quicklook
                if ql_dir:
                    base = f"ev{idx:04d}_{net}.{sta}.{loc or '--'}.{cha}.png"
                    out_png = os.path.join(ql_dir, base)
                    save_quicklook(tr, picks, out_png)

                # Record results
                if picks:
                    for k, p in enumerate(picks, 1):
                        rows.append({
                            **ev_meta,
                            "event_index": int(idx),
                            "net": net, "sta": sta, "loc": loc, "cha": cha,
                            "window_start": str(t1), "window_end": str(t2),
                            "trigger_rank": k,
                            "trigger_on_time": str(p["t_on"]),
                            "trigger_off_time": str(p["t_off"]),
                            "peak_cft": p["peak_cft"],
                            "dt_from_hint_s": p["dt_from_hint"],
                        })
                else:
                    rows.append({
                        **ev_meta,
                        "event_index": int(idx),
                        "net": net, "sta": sta, "loc": loc, "cha": cha,
                        "window_start": str(t1), "window_end": str(t2),
                        "trigger_rank": None,
                        "trigger_on_time": None,
                        "trigger_off_time": None,
                        "peak_cft": None,
                        "dt_from_hint_s": None,
                    })

            except Exception as e:
                rows.append({
                    **ev_meta,
                    "event_index": int(idx),
                    "net": net, "sta": sta, "loc": loc, "cha": cha,
                    "window_start": str(t1), "window_end": str(t2),
                    "error": str(e),
                })
                continue

    out_df = pd.DataFrame(rows)
    out_df.to_csv(args.out, index=False)
    print(f"Wrote results to {args.out}  (rows={len(out_df)})")


if __name__ == "__main__":
    main()
