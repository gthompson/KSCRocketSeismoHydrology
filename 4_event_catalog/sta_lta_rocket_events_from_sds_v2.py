#!/usr/bin/env python3
"""
sta_lta_rocket_events_from_sds.py

Loop through rocket launch time windows from a CSV, pad the windows,
pull data from an SDS archive using flovopy.sds.sds.SDSobj (or fallback to ObsPy SDSClient),
and run ObsPy STA/LTA to detect (a) the main launch signal and (b) a potential
booster landing signal returning to KSC.

Outputs a results CSV with detected trigger times and metrics per event/station.
Optionally saves quicklook PNGs.

CSV requirements (flexible):
- Either provide `start_time` and `end_time` columns (ISO-8601 or "YYYY-MM-DD HH:MM:SS")
- Or provide `t0` (launch epoch or ISO) plus `window_seconds` (duration)
Optional identifier columns (copied through): event_id, mission, rocket, pad, notes, etc.

Example:
    python sta_lta_rocket_events_from_sds.py \
        --csv /data/events.csv \
        --sds-root /data/remastered/SDS_KSC \
        --stations "1R.TANK.00.DHZ,1R.CARL1.00.DHZ" \
        --pad-before 60 --pad-after 600 \
        --sta 1.0 --lta 20.0 --on 3.5 --off 1.5 \
        --max-two-triggers \
        --out /data/results_sta_lta.csv \
        --quicklooks /data/quicklooks

Author: (generated by ChatGPT for Glenn)
"""

import os
import sys
import argparse
import warnings
from typing import List, Optional, Tuple
import traceback

import numpy as np
import pandas as pd
from obspy import UTCDateTime, read
from obspy.signal.trigger import classic_sta_lta, trigger_onset

from flovopy.sds.sds import SDSobj
from flovopy.processing.detection import add_sta_lta_triggers_to_stream, plot_stalta_triggers_on_stream

import matplotlib
matplotlib.use("Agg")  # for headless environments
import matplotlib.pyplot as plt


def parse_station_list(stations_str: str) -> List[Tuple[str, str, str, str]]:
    """
    Parse a comma-separated list of trace IDs into (net, sta, loc, cha) tuples.
    Accepts forms like:
        1R.TANK.00.DHZ
        1R.TANK..DHZ      (empty loc)
        AM.R37BE.00.HDF
    """
    items = []
    for part in stations_str.split(","):
        part = part.strip()
        if not part:
            continue
        fields = part.split(".")
        if len(fields) != 4:
            raise ValueError(f"Station spec must be NET.STA.LOC.CHA: got '{part}'")
        net, sta, loc, cha = fields
        loc = "" if loc in ("--", ".", "*", "  ", None) else loc
        items.append((net, sta, loc, cha))
    return items


def to_utc(val) -> UTCDateTime:
    """Robustly parse a time field that may be ISO string or epoch float/int."""
    if pd.isna(val):
        raise ValueError("Missing time value")
    # Try float epoch
    val=val[:19]
    try:
        return UTCDateTime(float(val))
    except Exception:
        # Try string
        return UTCDateTime(str(val))


def load_events(csv_path: str) -> pd.DataFrame:
    df = pd.read_csv(csv_path)
    # Normalize possible time columns. Support window_start/window_end used in Launch Library exports.
    if {"start_time", "end_time"} <= set(df.columns):
        df["t_start"] = df["start_time"].apply(to_utc)
        df["t_end"] = df["end_time"].apply(to_utc)
    elif {"window_start", "window_end"} <= set(df.columns):
        df["t_start"] = df["window_start"].apply(to_utc)
        df["t_end"] = df["window_end"].apply(to_utc)
    elif {"t0", "window_seconds"} <= set(df.columns):
        df["t0"] = df["t0"].apply(to_utc)
        df["t_start"] = df["t0"]
        df["t_end"] = df["t0"] + df["window_seconds"].astype(float)
    else:
        raise ValueError(
            "CSV must contain either start_time & end_time, window_start & window_end, or t0 & window_seconds"
        )
    return df


def get_stream_from_sds(sds_root: str, net: str, sta: str, loc: str, cha: str,
                        t1: UTCDateTime, t2: UTCDateTime, speed: int = 3):
    """
    Get an ObsPy Stream from SDS using flovopy.sds.sds.SDSobj if available,
    otherwise use ObsPy's SDSClient as a fallback.
    """

    try:
        sds = SDSobj(sds_root)
        # Many wrappers emulate ObsPy SDSClient
        st = sds.client.get_waveforms(net, sta, loc or "", cha, t1, t2, cleanup=True)
        return st
    except Exception as e:
        warnings.warn(f"SDSobj failed ({e})")

def run_sta_lta_on_trace(tr, sta_s: float, lta_s: float, on: float, off: float,
                         max_trigs: int = 2, t0_hint: Optional[UTCDateTime] = None,
                         min_sep_s: float = 30.0):
    """
    Run classic STA/LTA on a single Trace and return up to `max_trigs` trigger onsets.
    Returns a list of dicts with onset/offset indices & times and max STA/LTA value near onset.
    """
    sr = tr.stats.sampling_rate
    if sr is None or sr <= 0:
        return []

    nsta = max(1, int(round(sta_s * sr)))
    nlta = max(nsta + 1, int(round(lta_s * sr)))
    cft = classic_sta_lta(tr.data.astype(float), nsta, nlta)

    # Get all on/off pairs
    on_off = trigger_onset(cft, on, off)
    if on_off is None or len(on_off) == 0:
        return []

    # Rank triggers by peak CFT near onset
    candidates = []
    for (i_on, i_off) in on_off:
        i0 = max(0, i_on - int(1 * sr))
        i1 = min(len(cft) - 1, i_on + int(5 * sr))
        peak_val = float(np.nanmax(cft[i0:i1])) if i1 > i0 else float(np.nanmax(cft))
        t_on = tr.stats.starttime + (i_on / sr)
        t_off = tr.stats.starttime + (i_off / sr)
        dt_hint = abs((t_on - t0_hint)) if t0_hint else None
        candidates.append({
            "i_on": int(i_on), "i_off": int(i_off),
            "t_on": t_on, "t_off": t_off,
            "peak_cft": peak_val,
            "dt_from_hint": float(dt_hint) if dt_hint is not None else None
        })

    # Sort: prefer closeness to hint first (if provided), then peak_cft descending
    if t0_hint is not None:
        candidates.sort(key=lambda d: (d["dt_from_hint"], -d["peak_cft"]))
    else:
        candidates.sort(key=lambda d: -d["peak_cft"])

    # Pick up to max_trigs, enforcing min separation
    picks = []
    for cand in candidates:
        if not picks:
            picks.append(cand)
            continue
        if all(abs((cand["t_on"] - p["t_on"])) >= min_sep_s for p in picks):
            picks.append(cand)
        if len(picks) >= max_trigs:
            break

    return picks


def save_quicklook(tr, picks, out_png):
    """
    Save a simple waveform + STA/LTA trigger overlay quicklook.
    """
    try:
        sr = tr.stats.sampling_rate
        t = np.arange(tr.stats.npts) / sr
        fig = plt.figure(figsize=(10, 4))
        ax = plt.gca()
        ax.plot(t, tr.data, linewidth=0.8)
        for k, p in enumerate(picks, 1):
            ton = (p["t_on"] - tr.stats.starttime)
            ax.axvline(ton, linestyle="--", linewidth=1.0, label=f"Trig {k}")
        ax.set_xlabel("Time (s) since window start")
        ax.set_ylabel("Amplitude")
        ax.legend(loc="upper right")
        ax.set_title(f"{tr.id}  {tr.stats.starttime} to {tr.stats.endtime}")
        fig.tight_layout()
        fig.savefig(out_png, dpi=150)
        plt.close(fig)
    except Exception as e:
        warnings.warn(f"Quicklook save failed for {tr.id}: {e}")



def discover_all_trace_ids_with_sdsobj(sds_root: str, intervals, skip_low_rate_channels: bool=True, speed: int=3):
    """
    Discover non-empty trace IDs in the SDS archive over a set of (t1, t2) UTCDateTime intervals
    using flovopy.sds.sds.SDSobj._get_nonempty_traceids. Returns a sorted list of strings
    like 'NET.STA.LOC.CHA'.
    """

   
    found = set()
    for (start_epoch, end_epoch) in intervals:
        try:
            tids = sds._get_nonempty_traceids(start_epoch, end_epoch,
                                              skip_low_rate_channels=skip_low_rate_channels,
                                              speed=speed)
            print(f"For event {start_epoch}–{end_epoch}: {tids}")
            for tid in tids:
                found.add(tid)
        except Exception as e:
            import warnings
            warnings.warn(f"Trace discovery failed for interval {start_epoch}–{end_epoch}: {e}")
            continue
    return sorted(found)


def parse_trace_id_string(tid: str):
    """Split 'NET.STA.LOC.CHA' into (net, sta, loc, cha), normalizing empty loc."""
    parts = tid.split('.')
    if len(parts) != 4:
        raise ValueError(f"Bad trace id '{tid}' (expected NET.STA.LOC.CHA)")
    net, sta, loc, cha = parts
    loc = '' if loc in ('--', '.', '*', '  ', None) else loc
    return net, sta, loc, cha


def main():
    ap = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    ap.add_argument("--csv", required=True, help="CSV of event windows")
    ap.add_argument("--sds-root", required=True, help="Root of SDS archive")
    ap.add_argument("--stations", required=False,
                    help="Comma list of NET.STA.LOC.CHA (e.g., '1R.TANK.00.DHZ,1R.CARL1.00.DHZ')")
    ap.add_argument("--pad-before", type=float, default=60.0, help="Seconds before window start")
    ap.add_argument("--pad-after", type=float, default=600.0, help="Seconds after window end")
    ap.add_argument("--sta", type=float, default=1.0, help="STA window (s)")
    ap.add_argument("--lta", type=float, default=20.0, help="LTA window (s)")
    ap.add_argument("--on", type=float, default=3.5, help="Trigger on threshold")
    ap.add_argument("--off", type=float, default=1.5, help="Trigger off threshold")
    ap.add_argument("--max-two-triggers", action="store_true",
                    help="Return up to two triggers (e.g., launch + booster)")
    ap.add_argument("--min-sep", type=float, default=45.0,
                    help="Minimum separation (s) between triggers when selecting two")
    ap.add_argument("--quicklooks", default=None,
                    help="Directory to save quicklook PNGs (optional)")
    ap.add_argument("--out", required=True, help="Output results CSV path")
    args = ap.parse_args()

    # Load events
    df = load_events(args.csv)

    '''
    # Determine stations
    if args.stations:
        stations = parse_station_list(args.stations)
    else:
        # Build padded intervals from events
        intervals = []
        for _idx, _ev in df.iterrows():
            _t1 = _ev["t_start"] - args.pad_before
            _t2 = _ev["t_end"] + args.pad_after
            intervals.append((_t1, _t2))
        # Discover all trace ids with flovopy SDSobj
        tid_list = discover_all_trace_ids_with_sdsobj(args.sds_root, intervals, skip_low_rate_channels=True, speed=3)
        if not tid_list:
            raise RuntimeError("No non-empty trace IDs discovered in SDS for the provided windows.")
        stations = [parse_trace_id_string(tid) for tid in tid_list]
    '''

    # Prepare output
    rows = []
    os.makedirs(os.path.dirname(args.out), exist_ok=True)
    ql_dir = None
    if args.quicklooks:
        ql_dir = args.quicklooks
        os.makedirs(ql_dir, exist_ok=True)



    sdsin = SDSobj(args.sds_root)
    for idx, ev in df.iterrows():
        t1 = ev["t_start"] - args.pad_before
        t2 = ev["t_end"] + args.pad_after

        # Event IDs/labels to carry through (avoid collisions with trace keys like net/sta/loc/cha)
        reserved = {"t_start", "t_end", "net", "sta", "loc", "cha", "window_start", "window_end", "start_time", "end_time"}
        ev_meta = {}
        for col in df.columns:
            if col in {"t_start", "t_end"}:
                continue
            key = f"event_{col}" if col in reserved else col
            ev_meta[key] = ev[col]

        ids = sdsin._get_nonempty_traceids(t1, t2)
        #if len(ids)>0:
        try:
            sdsin.read(t1, t2, speed=2)
            st = sdsin.stream
            if len(st)==0:
                raise RuntimeError("Empty stream")
            st.detrend('linear')
            st.taper(0.03)
            picks_by_trace, picks_df = add_sta_lta_triggers_to_stream(
                st, sta_s=2.0, lta_s=10.0, on=2.0, off=1.0, max_trigs=2
            )
            print(picks_df)
            # Each tr now has tr.stats.triggers = [[UTC_on, UTC_off], ...]
            # and (if available) tr.stats.trigger_details = [{...}, ...]

            for tr in st:
                net, sta, loc, cha = tr.id.split(".")  # fix: use 'cha' consistently

                # Prefer detailed records if present
                details = getattr(tr.stats, "trigger_details", None)
                if details and len(details) > 0:
                    for k, p in enumerate(details, 1):
                        rows.append({
                            **ev_meta,
                            "event_index": int(idx),
                            "net": net, "sta": sta, "loc": loc, "cha": cha,
                            "window_start": str(t1), "window_end": str(t2),
                            "trigger_rank": k,
                            "trigger_on_time": str(p.get("t_on")),
                            "trigger_off_time": str(p.get("t_off")),
                            "peak_cft": p.get("peak_cft"),
                            "dt_from_hint_s": p.get("dt_from_hint"),
                        })
                    continue  # done with this trace

                # Fallback to simple on/off pairs
                pairs = getattr(tr.stats, "triggers", []) or []
                if pairs:
                    for k, (on_utc, off_utc) in enumerate(pairs, 1):
                        rows.append({
                            **ev_meta,
                            "event_index": int(idx),
                            "net": net, "sta": sta, "loc": loc, "cha": cha,
                            "window_start": str(t1), "window_end": str(t2),
                            "trigger_rank": k,
                            "trigger_on_time": str(on_utc),
                            "trigger_off_time": str(off_utc),
                            "peak_cft": None,
                            "dt_from_hint_s": None,
                        })
                else:
                    # No triggers for this trace
                    rows.append({
                        **ev_meta,
                        "event_index": int(idx),
                        "net": net, "sta": sta, "loc": loc, "cha": cha,
                        "window_start": str(t1), "window_end": str(t2),
                        "trigger_rank": None,
                        "trigger_on_time": None,
                        "trigger_off_time": None,
                        "peak_cft": None,
                        "dt_from_hint_s": None,
                    }


        except Exception as e:
            print(e)
            traceback.print_exc()

    out_df = pd.DataFrame(rows)
    out_df.to_csv(args.out, index=False)
    print(f"Wrote results to {args.out}  (rows={len(out_df)})")


if __name__ == "__main__":
    main()
